2025-02-02 06:16:52,800 - asyncio - DEBUG - Using selector: EpollSelector
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature exit
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature initialize
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature initialized
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature notebookDocument/didChange
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature notebookDocument/didClose
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature notebookDocument/didOpen
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature $/setTrace
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature shutdown
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature textDocument/didChange
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature textDocument/didClose
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature textDocument/didOpen
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature window/workDoneProgress/cancel
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature workspace/didChangeWorkspaceFolders
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered builtin feature workspace/executeCommand
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered "initialize" with options "None"
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered "textDocument/completion" with options "None"
2025-02-02 06:16:52,808 - pygls.feature_manager - INFO - Registered "textDocument/didChange" with options "None"
2025-02-02 06:16:52,808 - ollama_copilot - INFO - Starting Ollama LSP server
2025-02-02 06:16:52,808 - pygls.server - INFO - Starting IO server
2025-02-02 06:16:52,809 - pygls.server - DEBUG - Content length: 4170
2025-02-02 06:16:52,809 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 4170\r\n\r\n{"id":1,"params":{"workDoneToken":"1","trace":"off","initializationOptions":{"stream_suggestion":false,"model_name":"deepseek-coder:base","ollama_model_opts":{"temperature":0.1,"num_predict":40}},"rootUri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim","rootPath":"\\/home\\/elmortti\\/.config\\/nvim","clientInfo":{"name":"Neovim","version":"0.10.4+v0.10.4"},"processId":973784,"capabilities":{"textDocument":{"diagnostic":{"dynamicRegistration":false},"signatureHelp":{"dynamicRegistration":false,"signatureInformation":{"activeParameterSupport":true,"parameterInformation":{"labelOffsetSupport":true},"documentationFormat":["markdown","plaintext"]}},"synchronization":{"didSave":true,"willSave":true,"dynamicRegistration":false,"willSaveWaitUntil":true},"references":{"dynamicRegistration":false},"documentHighlight":{"dynamicRegistration":false},"documentSymbol":{"symbolKind":{"valueSet":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]},"dynamicRegistration":false,"hierarchicalDocumentSymbolSupport":true},"declaration":{"linkSupport":true},"definition":{"linkSupport":true,"dynamicRegistration":true},"publishDiagnostics":{"dataSupport":true,"relatedInformation":true,"tagSupport":{"valueSet":[1,2]}},"formatting":{"dynamicRegistration":true},"rangeFormatting":{"dynamicRegistration":true},"completion":{"completionList":{"itemDefaults":["commitCharacters","editRange","insertTextFormat","insertTextMode","data"]},"completionItem":{"snippetSupport":true,"commitCharactersSupport":true,"resolveSupport":{"properties":["documentation","additionalTextEdits","insertTextFormat","insertTextMode","command"]},"deprecatedSupport":true,"documentationFormat":["markdown","plaintext"],"tagSupport":{"valueSet":[1]},"labelDetailsSupport":true,"insertTextModeSupport":{"valueSet":[1,2]},"preselectSupport":true,"insertReplaceSupport":true},"dynamicRegistration":false,"insertTextMode":1,"contextSupport":true,"completionItemKind":{"valueSet":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]}},"inlayHint":{"dynamicRegistration":true,"resolveSupport":{"properties":["textEdits","tooltip","location","command"]}},"hover":{"dynamicRegistration":true,"contentFormat":["markdown","plaintext"]},"semanticTokens":{"dynamicRegistration":false,"multilineTokenSupport":false,"serverCancelSupport":false,"augmentsSyntaxTokens":true,"tokenModifiers":["declaration","definition","readonly","static","deprecated","abstract","async","modification","documentation","defaultLibrary"],"overlappingTokenSupport":true,"formats":["relative"],"tokenTypes":["namespace","type","class","enum","interface","struct","typeParameter","parameter","variable","property","enumMember","event","function","method","macro","keyword","modifier","comment","string","number","regexp","operator","decorator"],"requests":{"full":{"delta":true},"range":false}},"callHierarchy":{"dynamicRegistration":false},"rename":{"dynamicRegistration":true,"prepareSupport":true},"codeAction":{"codeActionLiteralSupport":{"codeActionKind":{"valueSet":["","quickfix","refactor","refactor.extract","refactor.inline","refactor.rewrite","source","source.organizeImports"]}},"dynamicRegistration":true,"isPreferredSupport":true,"dataSupport":true,"resolveSupport":{"properties":["edit"]}},"typeDefinition":{"linkSupport":true},"implementation":{"linkSupport":true}},"workspace":{"configuration":true,"symbol":{"dynamicRegistration":false,"symbolKind":{"valueSet":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]}},"workspaceFolders":true,"applyEdit":true,"workspaceEdit":{"resourceOperations":["rename","create","delete"]},"didChangeWatchedFiles":{"dynamicRegistration":false,"relativePatternSupport":true},"semanticTokens":{"refreshSupport":true},"inlayHint":{"refreshSupport":true},"didChangeConfiguration":{"dynamicRegistration":false}},"window":{"showMessage":{"messageActionItem":{"additionalPropertiesSupport":false}},"showDocument":{"support":true},"workDoneProgress":true},"general":{"positionEncodings":["utf-16"]}},"workspaceFolders":[{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim","name":"\\/home\\/elmortti\\/.config\\/nvim"}]},"method":"initialize","jsonrpc":"2.0"}'
2025-02-02 06:16:52,825 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:16:52,825 - pygls.protocol.language_server - INFO - Language server initialized InitializeParams(capabilities=ClientCapabilities(workspace=WorkspaceClientCapabilities(apply_edit=True, workspace_edit=WorkspaceEditClientCapabilities(document_changes=None, resource_operations=[<ResourceOperationKind.Rename: 'rename'>, <ResourceOperationKind.Create: 'create'>, <ResourceOperationKind.Delete: 'delete'>], failure_handling=None, normalizes_line_endings=None, change_annotation_support=None), did_change_configuration=DidChangeConfigurationClientCapabilities(dynamic_registration=False), did_change_watched_files=DidChangeWatchedFilesClientCapabilities(dynamic_registration=False, relative_pattern_support=True), symbol=WorkspaceSymbolClientCapabilities(dynamic_registration=False, symbol_kind=WorkspaceSymbolClientCapabilitiesSymbolKindType(value_set=[<SymbolKind.File: 1>, <SymbolKind.Module: 2>, <SymbolKind.Namespace: 3>, <SymbolKind.Package: 4>, <SymbolKind.Class: 5>, <SymbolKind.Method: 6>, <SymbolKind.Property: 7>, <SymbolKind.Field: 8>, <SymbolKind.Constructor: 9>, <SymbolKind.Enum: 10>, <SymbolKind.Interface: 11>, <SymbolKind.Function: 12>, <SymbolKind.Variable: 13>, <SymbolKind.Constant: 14>, <SymbolKind.String: 15>, <SymbolKind.Number: 16>, <SymbolKind.Boolean: 17>, <SymbolKind.Array: 18>, <SymbolKind.Object: 19>, <SymbolKind.Key: 20>, <SymbolKind.Null: 21>, <SymbolKind.EnumMember: 22>, <SymbolKind.Struct: 23>, <SymbolKind.Event: 24>, <SymbolKind.Operator: 25>, <SymbolKind.TypeParameter: 26>]), tag_support=None, resolve_support=None), execute_command=None, workspace_folders=True, configuration=True, semantic_tokens=SemanticTokensWorkspaceClientCapabilities(refresh_support=True), code_lens=None, file_operations=None, inline_value=None, inlay_hint=InlayHintWorkspaceClientCapabilities(refresh_support=True), diagnostics=None, folding_range=None), text_document=TextDocumentClientCapabilities(synchronization=TextDocumentSyncClientCapabilities(dynamic_registration=False, will_save=True, will_save_wait_until=True, did_save=True), completion=CompletionClientCapabilities(dynamic_registration=False, completion_item=CompletionClientCapabilitiesCompletionItemType(snippet_support=True, commit_characters_support=True, documentation_format=[<MarkupKind.Markdown: 'markdown'>, <MarkupKind.PlainText: 'plaintext'>], deprecated_support=True, preselect_support=True, tag_support=CompletionClientCapabilitiesCompletionItemTypeTagSupportType(value_set=[<CompletionItemTag.Deprecated: 1>]), insert_replace_support=True, resolve_support=CompletionClientCapabilitiesCompletionItemTypeResolveSupportType(properties=['documentation', 'additionalTextEdits', 'insertTextFormat', 'insertTextMode', 'command']), insert_text_mode_support=CompletionClientCapabilitiesCompletionItemTypeInsertTextModeSupportType(value_set=[<InsertTextMode.AsIs: 1>, <InsertTextMode.AdjustIndentation: 2>]), label_details_support=True), completion_item_kind=CompletionClientCapabilitiesCompletionItemKindType(value_set=[<CompletionItemKind.Text: 1>, <CompletionItemKind.Method: 2>, <CompletionItemKind.Function: 3>, <CompletionItemKind.Constructor: 4>, <CompletionItemKind.Field: 5>, <CompletionItemKind.Variable: 6>, <CompletionItemKind.Class: 7>, <CompletionItemKind.Interface: 8>, <CompletionItemKind.Module: 9>, <CompletionItemKind.Property: 10>, <CompletionItemKind.Unit: 11>, <CompletionItemKind.Value: 12>, <CompletionItemKind.Enum: 13>, <CompletionItemKind.Keyword: 14>, <CompletionItemKind.Snippet: 15>, <CompletionItemKind.Color: 16>, <CompletionItemKind.File: 17>, <CompletionItemKind.Reference: 18>, <CompletionItemKind.Folder: 19>, <CompletionItemKind.EnumMember: 20>, <CompletionItemKind.Constant: 21>, <CompletionItemKind.Struct: 22>, <CompletionItemKind.Event: 23>, <CompletionItemKind.Operator: 24>, <CompletionItemKind.TypeParameter: 25>]), insert_text_mode=<InsertTextMode.AsIs: 1>, context_support=True, completion_list=CompletionClientCapabilitiesCompletionListType(item_defaults=['commitCharacters', 'editRange', 'insertTextFormat', 'insertTextMode', 'data'])), hover=HoverClientCapabilities(dynamic_registration=True, content_format=[<MarkupKind.Markdown: 'markdown'>, <MarkupKind.PlainText: 'plaintext'>]), signature_help=SignatureHelpClientCapabilities(dynamic_registration=False, signature_information=SignatureHelpClientCapabilitiesSignatureInformationType(documentation_format=[<MarkupKind.Markdown: 'markdown'>, <MarkupKind.PlainText: 'plaintext'>], parameter_information=SignatureHelpClientCapabilitiesSignatureInformationTypeParameterInformationType(label_offset_support=True), active_parameter_support=True), context_support=None), declaration=DeclarationClientCapabilities(dynamic_registration=None, link_support=True), definition=DefinitionClientCapabilities(dynamic_registration=True, link_support=True), type_definition=TypeDefinitionClientCapabilities(dynamic_registration=None, link_support=True), implementation=ImplementationClientCapabilities(dynamic_registration=None, link_support=True), references=ReferenceClientCapabilities(dynamic_registration=False), document_highlight=DocumentHighlightClientCapabilities(dynamic_registration=False), document_symbol=DocumentSymbolClientCapabilities(dynamic_registration=False, symbol_kind=DocumentSymbolClientCapabilitiesSymbolKindType(value_set=[<SymbolKind.File: 1>, <SymbolKind.Module: 2>, <SymbolKind.Namespace: 3>, <SymbolKind.Package: 4>, <SymbolKind.Class: 5>, <SymbolKind.Method: 6>, <SymbolKind.Property: 7>, <SymbolKind.Field: 8>, <SymbolKind.Constructor: 9>, <SymbolKind.Enum: 10>, <SymbolKind.Interface: 11>, <SymbolKind.Function: 12>, <SymbolKind.Variable: 13>, <SymbolKind.Constant: 14>, <SymbolKind.String: 15>, <SymbolKind.Number: 16>, <SymbolKind.Boolean: 17>, <SymbolKind.Array: 18>, <SymbolKind.Object: 19>, <SymbolKind.Key: 20>, <SymbolKind.Null: 21>, <SymbolKind.EnumMember: 22>, <SymbolKind.Struct: 23>, <SymbolKind.Event: 24>, <SymbolKind.Operator: 25>, <SymbolKind.TypeParameter: 26>]), hierarchical_document_symbol_support=True, tag_support=None, label_support=None), code_action=CodeActionClientCapabilities(dynamic_registration=True, code_action_literal_support=CodeActionClientCapabilitiesCodeActionLiteralSupportType(code_action_kind=CodeActionClientCapabilitiesCodeActionLiteralSupportTypeCodeActionKindType(value_set=['', 'quickfix', 'refactor', 'refactor.extract', 'refactor.inline', 'refactor.rewrite', 'source', 'source.organizeImports'])), is_preferred_support=True, disabled_support=None, data_support=True, resolve_support=CodeActionClientCapabilitiesResolveSupportType(properties=['edit']), honors_change_annotations=None), code_lens=None, document_link=None, color_provider=None, formatting=DocumentFormattingClientCapabilities(dynamic_registration=True), range_formatting=DocumentRangeFormattingClientCapabilities(dynamic_registration=True, ranges_support=None), on_type_formatting=None, rename=RenameClientCapabilities(dynamic_registration=True, prepare_support=True, prepare_support_default_behavior=None, honors_change_annotations=None), folding_range=None, selection_range=None, publish_diagnostics=PublishDiagnosticsClientCapabilities(related_information=True, tag_support=PublishDiagnosticsClientCapabilitiesTagSupportType(value_set=[<DiagnosticTag.Unnecessary: 1>, <DiagnosticTag.Deprecated: 2>]), version_support=None, code_description_support=None, data_support=True), call_hierarchy=CallHierarchyClientCapabilities(dynamic_registration=False), semantic_tokens=SemanticTokensClientCapabilities(requests=SemanticTokensClientCapabilitiesRequestsType(range=False, full=SemanticTokensClientCapabilitiesRequestsTypeFullType1(delta=True)), token_types=['namespace', 'type', 'class', 'enum', 'interface', 'struct', 'typeParameter', 'parameter', 'variable', 'property', 'enumMember', 'event', 'function', 'method', 'macro', 'keyword', 'modifier', 'comment', 'string', 'number', 'regexp', 'operator', 'decorator'], token_modifiers=['declaration', 'definition', 'readonly', 'static', 'deprecated', 'abstract', 'async', 'modification', 'documentation', 'defaultLibrary'], formats=[<TokenFormat.Relative: 'relative'>], dynamic_registration=False, overlapping_token_support=True, multiline_token_support=False, server_cancel_support=False, augments_syntax_tokens=True), linked_editing_range=None, moniker=None, type_hierarchy=None, inline_value=None, inlay_hint=InlayHintClientCapabilities(dynamic_registration=True, resolve_support=InlayHintClientCapabilitiesResolveSupportType(properties=['textEdits', 'tooltip', 'location', 'command'])), diagnostic=DiagnosticClientCapabilities(dynamic_registration=False, related_document_support=None), inline_completion=None), notebook_document=None, window=WindowClientCapabilities(work_done_progress=True, show_message=ShowMessageRequestClientCapabilities(message_action_item=ShowMessageRequestClientCapabilitiesMessageActionItemType(additional_properties_support=False)), show_document=ShowDocumentClientCapabilities(support=True)), general=GeneralClientCapabilities(stale_request_support=None, regular_expressions=None, markdown=None, position_encodings=['utf-16']), experimental=None), process_id=973784, client_info=InitializeParamsClientInfoType(name='Neovim', version='0.10.4+v0.10.4'), locale=None, root_path='/home/elmortti/.config/nvim', root_uri='file:///home/elmortti/.config/nvim', initialization_options={'stream_suggestion': False, 'model_name': 'deepseek-coder:base', 'ollama_model_opts': {'temperature': 0.1, 'num_predict': 40}}, trace=<TraceValues.Off: 'off'>, work_done_token='1', workspace_folders=[WorkspaceFolder(uri='file:///home/elmortti/.config/nvim', name='/home/elmortti/.config/nvim')])
2025-02-02 06:16:52,829 - pygls.protocol.language_server - DEBUG - Server capabilities: {"positionEncoding": "utf-16", "textDocumentSync": {"openClose": true, "change": 2, "willSave": false, "willSaveWaitUntil": false, "save": false}, "completionProvider": {}, "executeCommandProvider": {"commands": []}, "workspace": {"workspaceFolders": {"supported": true, "changeNotifications": true}, "fileOperations": {}}}
2025-02-02 06:16:52,829 - ollama_copilot - INFO - Initializing Ollama LSP server
2025-02-02 06:16:52,832 - ollama_copilot - INFO - Initialized with model: deepseek-coder:base
2025-02-02 06:16:52,834 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 1, "jsonrpc": "2.0", "result": {"capabilities": {"positionEncoding": "utf-16", "textDocumentSync": {"openClose": true, "change": 2, "willSave": false, "willSaveWaitUntil": false, "save": false}, "completionProvider": {}, "executeCommandProvider": {"commands": []}, "workspace": {"workspaceFolders": {"supported": true, "changeNotifications": true}, "fileOperations": {}}}, "serverInfo": {"name": "ollama-server", "version": "v0.3"}}}
2025-02-02 06:16:52,835 - pygls.server - DEBUG - Content length: 52
2025-02-02 06:16:52,835 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 52\r\n\r\n{"params":{},"jsonrpc":"2.0","method":"initialized"}'
2025-02-02 06:16:52,835 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:16:52,836 - pygls.server - DEBUG - Content length: 677
2025-02-02 06:16:52,836 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 677\r\n\r\n{"params":{"textDocument":{"languageId":"lua","uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":0,"text":"-- Custom configuration (defaults shown)\\nreturn {\\n\\t\'jasper-clarke\\/collama.nvim\',\\n\\topts = {\\n\\t\\tmodel_name = \\"deepseek-coder:base\\",\\n\\t\\tstream_suggestion = false,\\n\\t\\tpython_command = \\"python3\\",\\n\\t\\tfiletypes = { \'python\', \'lua\', \'vim\', \\"markdown\\" },\\n\\t\\tollama_model_opts = {\\n\\t\\t\\tnum_predict = 40,\\n\\t\\t\\ttemperature = 0.1,\\n\\t\\t},\\n\\t\\tkeymaps = {\\n\\t\\t\\tsuggestion = \'<leader>os\',\\n\\t\\t\\treject = \'<leader>or\',\\n\\t\\t\\tinsert_accept = \'<Tab>\',\\n\\t\\t},\\n\\t}\\n}\\n"}},"jsonrpc":"2.0","method":"textDocument\\/didOpen"}'
2025-02-02 06:16:52,837 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:00,639 - pygls.server - DEBUG - Content length: 301
2025-02-02 06:17:00,640 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 301\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":3},"contentChanges":[{"range":{"end":{"character":27,"line":15},"start":{"character":27,"line":15}},"rangeLength":0,"text":"\\n\\t\\t\\t"}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:00,645 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:00,646 - ollama_copilot - DEBUG - Document change detected: 
			
2025-02-02 06:17:02,451 - pygls.server - DEBUG - Content length: 291
2025-02-02 06:17:02,452 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 291\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":4},"contentChanges":[{"range":{"end":{"character":3,"line":16},"start":{"character":2,"line":16}},"rangeLength":1,"text":""}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:02,452 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:02,453 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:02,455 - pygls.server - DEBUG - Content length: 395
2025-02-02 06:17:02,456 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 395\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":6},"contentChanges":[{"range":{"end":{"character":2,"line":16},"start":{"character":1,"line":16}},"rangeLength":1,"text":""},{"range":{"end":{"character":1,"line":16},"start":{"character":0,"line":16}},"rangeLength":1,"text":""}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:02,456 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:02,457 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:02,776 - pygls.server - DEBUG - Content length: 201
2025-02-02 06:17:02,777 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 201\r\n\r\n{"id":2,"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"position":{"character":0,"line":16}},"method":"textDocument\\/completion","jsonrpc":"2.0"}'
2025-02-02 06:17:02,784 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:02,784 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 2, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:04,869 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:04,870 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":7},"contentChanges":[{"range":{"end":{"character":27,"line":15},"start":{"character":27,"line":15}},"rangeLength":0,"text":""}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:04,870 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:04,870 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:04,885 - pygls.server - DEBUG - Content length: 291
2025-02-02 06:17:04,885 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 291\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":8},"contentChanges":[{"range":{"end":{"character":0,"line":17},"start":{"character":0,"line":16}},"rangeLength":1,"text":""}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:04,886 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:04,886 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:05,216 - pygls.server - DEBUG - Content length: 301
2025-02-02 06:17:05,216 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 301\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":9},"contentChanges":[{"range":{"end":{"character":27,"line":15},"start":{"character":27,"line":15}},"rangeLength":0,"text":"\\n\\t\\t\\t"}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:05,216 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:05,216 - ollama_copilot - DEBUG - Document change detected: 
			
2025-02-02 06:17:13,726 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:13,726 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":10},"contentChanges":[{"range":{"end":{"character":3,"line":16},"start":{"character":2,"line":16}},"rangeLength":1,"text":""}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:13,726 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:13,726 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:13,727 - pygls.server - DEBUG - Content length: 396
2025-02-02 06:17:13,727 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 396\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":12},"contentChanges":[{"range":{"end":{"character":2,"line":16},"start":{"character":1,"line":16}},"rangeLength":1,"text":""},{"range":{"end":{"character":1,"line":16},"start":{"character":0,"line":16}},"rangeLength":1,"text":""}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:13,727 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:13,728 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:14,994 - pygls.server - DEBUG - Content length: 201
2025-02-02 06:17:14,994 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 201\r\n\r\n{"id":3,"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"position":{"character":0,"line":16}},"method":"textDocument\\/completion","jsonrpc":"2.0"}'
2025-02-02 06:17:14,994 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:14,994 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 3, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:21,677 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:17:21,677 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":13},"contentChanges":[{"range":{"end":{"character":26,"line":5},"start":{"character":22,"line":5}},"rangeLength":4,"text":"tru"}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:21,677 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:21,677 - ollama_copilot - DEBUG - Document change detected: tru
2025-02-02 06:17:24,376 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:24,377 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":14},"contentChanges":[{"range":{"end":{"character":35,"line":4},"start":{"character":31,"line":4}},"rangeLength":4,"text":""}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:24,377 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:24,377 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:24,929 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:24,930 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":15},"contentChanges":[{"range":{"end":{"character":31,"line":4},"start":{"character":31,"line":4}},"rangeLength":0,"text":"1"}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:24,930 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:24,931 - ollama_copilot - DEBUG - Document change detected: 1
2025-02-02 06:17:24,936 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:17:24,936 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"id":4,"params":{"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"position":{"character":32,"line":4}},"method":"textDocument\\/completion","jsonrpc":"2.0"}'
2025-02-02 06:17:24,937 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:24,937 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 4, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:25,106 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:25,107 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":16},"contentChanges":[{"range":{"end":{"character":32,"line":4},"start":{"character":32,"line":4}},"rangeLength":0,"text":"."}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:25,107 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:25,107 - ollama_copilot - DEBUG - Document change detected: .
2025-02-02 06:17:25,187 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:25,187 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":17},"contentChanges":[{"range":{"end":{"character":33,"line":4},"start":{"character":33,"line":4}},"rangeLength":0,"text":"3"}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:25,187 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:25,187 - ollama_copilot - DEBUG - Document change detected: 3
2025-02-02 06:17:25,187 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:17:25,188 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"id":5,"params":{"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"position":{"character":34,"line":4}},"method":"textDocument\\/completion","jsonrpc":"2.0"}'
2025-02-02 06:17:25,188 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:25,188 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 5, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:25,506 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:25,506 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","version":18},"contentChanges":[{"range":{"end":{"character":34,"line":4},"start":{"character":34,"line":4}},"rangeLength":0,"text":"b"}]},"jsonrpc":"2.0","method":"textDocument\\/didChange"}'
2025-02-02 06:17:25,507 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:25,507 - ollama_copilot - DEBUG - Document change detected: b
2025-02-02 06:17:25,509 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:17:25,510 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"id":6,"params":{"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"position":{"character":35,"line":4}},"method":"textDocument\\/completion","jsonrpc":"2.0"}'
2025-02-02 06:17:25,510 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:25,510 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 6, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:31,913 - pygls.server - DEBUG - Content length: 44
2025-02-02 06:17:31,915 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 44\r\n\r\n{"id":7,"method":"shutdown","jsonrpc":"2.0"}'
2025-02-02 06:17:31,917 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:31,918 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 7, "jsonrpc": "2.0", "result": null}
2025-02-02 06:17:31,927 - pygls.server - DEBUG - Content length: 33
2025-02-02 06:17:31,927 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 33\r\n\r\n{"jsonrpc":"2.0","method":"exit"}'
2025-02-02 06:17:31,928 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:31,929 - pygls.server - INFO - Shutting down the server
2025-02-02 06:17:31,929 - pygls.server - INFO - Closing the event loop.
2025-02-02 06:17:37,826 - asyncio - DEBUG - Using selector: EpollSelector
2025-02-02 06:17:37,834 - pygls.feature_manager - INFO - Registered builtin feature exit
2025-02-02 06:17:37,834 - pygls.feature_manager - INFO - Registered builtin feature initialize
2025-02-02 06:17:37,834 - pygls.feature_manager - INFO - Registered builtin feature initialized
2025-02-02 06:17:37,834 - pygls.feature_manager - INFO - Registered builtin feature notebookDocument/didChange
2025-02-02 06:17:37,834 - pygls.feature_manager - INFO - Registered builtin feature notebookDocument/didClose
2025-02-02 06:17:37,834 - pygls.feature_manager - INFO - Registered builtin feature notebookDocument/didOpen
2025-02-02 06:17:37,834 - pygls.feature_manager - INFO - Registered builtin feature $/setTrace
2025-02-02 06:17:37,834 - pygls.feature_manager - INFO - Registered builtin feature shutdown
2025-02-02 06:17:37,834 - pygls.feature_manager - INFO - Registered builtin feature textDocument/didChange
2025-02-02 06:17:37,835 - pygls.feature_manager - INFO - Registered builtin feature textDocument/didClose
2025-02-02 06:17:37,835 - pygls.feature_manager - INFO - Registered builtin feature textDocument/didOpen
2025-02-02 06:17:37,835 - pygls.feature_manager - INFO - Registered builtin feature window/workDoneProgress/cancel
2025-02-02 06:17:37,835 - pygls.feature_manager - INFO - Registered builtin feature workspace/didChangeWorkspaceFolders
2025-02-02 06:17:37,835 - pygls.feature_manager - INFO - Registered builtin feature workspace/executeCommand
2025-02-02 06:17:37,835 - pygls.feature_manager - INFO - Registered "initialize" with options "None"
2025-02-02 06:17:37,835 - pygls.feature_manager - INFO - Registered "textDocument/completion" with options "None"
2025-02-02 06:17:37,835 - pygls.feature_manager - INFO - Registered "textDocument/didChange" with options "None"
2025-02-02 06:17:37,835 - ollama_copilot - INFO - Starting Ollama LSP server
2025-02-02 06:17:37,835 - pygls.server - INFO - Starting IO server
2025-02-02 06:17:37,835 - pygls.server - DEBUG - Content length: 4170
2025-02-02 06:17:37,836 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 4170\r\n\r\n{"method":"initialize","jsonrpc":"2.0","id":1,"params":{"trace":"off","processId":975954,"workspaceFolders":[{"name":"\\/home\\/elmortti\\/.config\\/nvim","uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim"}],"capabilities":{"general":{"positionEncodings":["utf-16"]},"workspace":{"semanticTokens":{"refreshSupport":true},"didChangeConfiguration":{"dynamicRegistration":false},"workspaceFolders":true,"applyEdit":true,"workspaceEdit":{"resourceOperations":["rename","create","delete"]},"didChangeWatchedFiles":{"relativePatternSupport":true,"dynamicRegistration":false},"inlayHint":{"refreshSupport":true},"symbol":{"symbolKind":{"valueSet":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]},"dynamicRegistration":false},"configuration":true},"window":{"showMessage":{"messageActionItem":{"additionalPropertiesSupport":false}},"showDocument":{"support":true},"workDoneProgress":true},"textDocument":{"rangeFormatting":{"dynamicRegistration":true},"completion":{"completionItemKind":{"valueSet":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]},"completionList":{"itemDefaults":["commitCharacters","editRange","insertTextFormat","insertTextMode","data"]},"completionItem":{"snippetSupport":true,"commitCharactersSupport":true,"preselectSupport":true,"deprecatedSupport":true,"documentationFormat":["markdown","plaintext"],"labelDetailsSupport":true,"insertTextModeSupport":{"valueSet":[1,2]},"insertReplaceSupport":true,"resolveSupport":{"properties":["documentation","additionalTextEdits","insertTextFormat","insertTextMode","command"]},"tagSupport":{"valueSet":[1]}},"dynamicRegistration":false,"insertTextMode":1,"contextSupport":true},"hover":{"contentFormat":["markdown","plaintext"],"dynamicRegistration":true},"declaration":{"linkSupport":true},"definition":{"dynamicRegistration":true,"linkSupport":true},"inlayHint":{"resolveSupport":{"properties":["textEdits","tooltip","location","command"]},"dynamicRegistration":true},"implementation":{"linkSupport":true},"typeDefinition":{"linkSupport":true},"semanticTokens":{"requests":{"full":{"delta":true},"range":false},"tokenTypes":["namespace","type","class","enum","interface","struct","typeParameter","parameter","variable","property","enumMember","event","function","method","macro","keyword","modifier","comment","string","number","regexp","operator","decorator"],"multilineTokenSupport":false,"serverCancelSupport":false,"augmentsSyntaxTokens":true,"dynamicRegistration":false,"tokenModifiers":["declaration","definition","readonly","static","deprecated","abstract","async","modification","documentation","defaultLibrary"],"formats":["relative"],"overlappingTokenSupport":true},"signatureHelp":{"signatureInformation":{"activeParameterSupport":true,"parameterInformation":{"labelOffsetSupport":true},"documentationFormat":["markdown","plaintext"]},"dynamicRegistration":false},"documentHighlight":{"dynamicRegistration":false},"synchronization":{"willSave":true,"didSave":true,"willSaveWaitUntil":true,"dynamicRegistration":false},"references":{"dynamicRegistration":false},"codeAction":{"codeActionLiteralSupport":{"codeActionKind":{"valueSet":["","quickfix","refactor","refactor.extract","refactor.inline","refactor.rewrite","source","source.organizeImports"]}},"dynamicRegistration":true,"isPreferredSupport":true,"dataSupport":true,"resolveSupport":{"properties":["edit"]}},"documentSymbol":{"hierarchicalDocumentSymbolSupport":true,"dynamicRegistration":false,"symbolKind":{"valueSet":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]}},"callHierarchy":{"dynamicRegistration":false},"diagnostic":{"dynamicRegistration":false},"rename":{"prepareSupport":true,"dynamicRegistration":true},"publishDiagnostics":{"relatedInformation":true,"dataSupport":true,"tagSupport":{"valueSet":[1,2]}},"formatting":{"dynamicRegistration":true}}},"workDoneToken":"1","initializationOptions":{"model_name":"deepseek-coder:base","ollama_model_opts":{"num_predict":40,"temperature":0.1},"stream_suggestion":false},"rootUri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim","rootPath":"\\/home\\/elmortti\\/.config\\/nvim","clientInfo":{"name":"Neovim","version":"0.10.4+v0.10.4"}}}'
2025-02-02 06:17:37,854 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:37,855 - pygls.protocol.language_server - INFO - Language server initialized InitializeParams(capabilities=ClientCapabilities(workspace=WorkspaceClientCapabilities(apply_edit=True, workspace_edit=WorkspaceEditClientCapabilities(document_changes=None, resource_operations=[<ResourceOperationKind.Rename: 'rename'>, <ResourceOperationKind.Create: 'create'>, <ResourceOperationKind.Delete: 'delete'>], failure_handling=None, normalizes_line_endings=None, change_annotation_support=None), did_change_configuration=DidChangeConfigurationClientCapabilities(dynamic_registration=False), did_change_watched_files=DidChangeWatchedFilesClientCapabilities(dynamic_registration=False, relative_pattern_support=True), symbol=WorkspaceSymbolClientCapabilities(dynamic_registration=False, symbol_kind=WorkspaceSymbolClientCapabilitiesSymbolKindType(value_set=[<SymbolKind.File: 1>, <SymbolKind.Module: 2>, <SymbolKind.Namespace: 3>, <SymbolKind.Package: 4>, <SymbolKind.Class: 5>, <SymbolKind.Method: 6>, <SymbolKind.Property: 7>, <SymbolKind.Field: 8>, <SymbolKind.Constructor: 9>, <SymbolKind.Enum: 10>, <SymbolKind.Interface: 11>, <SymbolKind.Function: 12>, <SymbolKind.Variable: 13>, <SymbolKind.Constant: 14>, <SymbolKind.String: 15>, <SymbolKind.Number: 16>, <SymbolKind.Boolean: 17>, <SymbolKind.Array: 18>, <SymbolKind.Object: 19>, <SymbolKind.Key: 20>, <SymbolKind.Null: 21>, <SymbolKind.EnumMember: 22>, <SymbolKind.Struct: 23>, <SymbolKind.Event: 24>, <SymbolKind.Operator: 25>, <SymbolKind.TypeParameter: 26>]), tag_support=None, resolve_support=None), execute_command=None, workspace_folders=True, configuration=True, semantic_tokens=SemanticTokensWorkspaceClientCapabilities(refresh_support=True), code_lens=None, file_operations=None, inline_value=None, inlay_hint=InlayHintWorkspaceClientCapabilities(refresh_support=True), diagnostics=None, folding_range=None), text_document=TextDocumentClientCapabilities(synchronization=TextDocumentSyncClientCapabilities(dynamic_registration=False, will_save=True, will_save_wait_until=True, did_save=True), completion=CompletionClientCapabilities(dynamic_registration=False, completion_item=CompletionClientCapabilitiesCompletionItemType(snippet_support=True, commit_characters_support=True, documentation_format=[<MarkupKind.Markdown: 'markdown'>, <MarkupKind.PlainText: 'plaintext'>], deprecated_support=True, preselect_support=True, tag_support=CompletionClientCapabilitiesCompletionItemTypeTagSupportType(value_set=[<CompletionItemTag.Deprecated: 1>]), insert_replace_support=True, resolve_support=CompletionClientCapabilitiesCompletionItemTypeResolveSupportType(properties=['documentation', 'additionalTextEdits', 'insertTextFormat', 'insertTextMode', 'command']), insert_text_mode_support=CompletionClientCapabilitiesCompletionItemTypeInsertTextModeSupportType(value_set=[<InsertTextMode.AsIs: 1>, <InsertTextMode.AdjustIndentation: 2>]), label_details_support=True), completion_item_kind=CompletionClientCapabilitiesCompletionItemKindType(value_set=[<CompletionItemKind.Text: 1>, <CompletionItemKind.Method: 2>, <CompletionItemKind.Function: 3>, <CompletionItemKind.Constructor: 4>, <CompletionItemKind.Field: 5>, <CompletionItemKind.Variable: 6>, <CompletionItemKind.Class: 7>, <CompletionItemKind.Interface: 8>, <CompletionItemKind.Module: 9>, <CompletionItemKind.Property: 10>, <CompletionItemKind.Unit: 11>, <CompletionItemKind.Value: 12>, <CompletionItemKind.Enum: 13>, <CompletionItemKind.Keyword: 14>, <CompletionItemKind.Snippet: 15>, <CompletionItemKind.Color: 16>, <CompletionItemKind.File: 17>, <CompletionItemKind.Reference: 18>, <CompletionItemKind.Folder: 19>, <CompletionItemKind.EnumMember: 20>, <CompletionItemKind.Constant: 21>, <CompletionItemKind.Struct: 22>, <CompletionItemKind.Event: 23>, <CompletionItemKind.Operator: 24>, <CompletionItemKind.TypeParameter: 25>]), insert_text_mode=<InsertTextMode.AsIs: 1>, context_support=True, completion_list=CompletionClientCapabilitiesCompletionListType(item_defaults=['commitCharacters', 'editRange', 'insertTextFormat', 'insertTextMode', 'data'])), hover=HoverClientCapabilities(dynamic_registration=True, content_format=[<MarkupKind.Markdown: 'markdown'>, <MarkupKind.PlainText: 'plaintext'>]), signature_help=SignatureHelpClientCapabilities(dynamic_registration=False, signature_information=SignatureHelpClientCapabilitiesSignatureInformationType(documentation_format=[<MarkupKind.Markdown: 'markdown'>, <MarkupKind.PlainText: 'plaintext'>], parameter_information=SignatureHelpClientCapabilitiesSignatureInformationTypeParameterInformationType(label_offset_support=True), active_parameter_support=True), context_support=None), declaration=DeclarationClientCapabilities(dynamic_registration=None, link_support=True), definition=DefinitionClientCapabilities(dynamic_registration=True, link_support=True), type_definition=TypeDefinitionClientCapabilities(dynamic_registration=None, link_support=True), implementation=ImplementationClientCapabilities(dynamic_registration=None, link_support=True), references=ReferenceClientCapabilities(dynamic_registration=False), document_highlight=DocumentHighlightClientCapabilities(dynamic_registration=False), document_symbol=DocumentSymbolClientCapabilities(dynamic_registration=False, symbol_kind=DocumentSymbolClientCapabilitiesSymbolKindType(value_set=[<SymbolKind.File: 1>, <SymbolKind.Module: 2>, <SymbolKind.Namespace: 3>, <SymbolKind.Package: 4>, <SymbolKind.Class: 5>, <SymbolKind.Method: 6>, <SymbolKind.Property: 7>, <SymbolKind.Field: 8>, <SymbolKind.Constructor: 9>, <SymbolKind.Enum: 10>, <SymbolKind.Interface: 11>, <SymbolKind.Function: 12>, <SymbolKind.Variable: 13>, <SymbolKind.Constant: 14>, <SymbolKind.String: 15>, <SymbolKind.Number: 16>, <SymbolKind.Boolean: 17>, <SymbolKind.Array: 18>, <SymbolKind.Object: 19>, <SymbolKind.Key: 20>, <SymbolKind.Null: 21>, <SymbolKind.EnumMember: 22>, <SymbolKind.Struct: 23>, <SymbolKind.Event: 24>, <SymbolKind.Operator: 25>, <SymbolKind.TypeParameter: 26>]), hierarchical_document_symbol_support=True, tag_support=None, label_support=None), code_action=CodeActionClientCapabilities(dynamic_registration=True, code_action_literal_support=CodeActionClientCapabilitiesCodeActionLiteralSupportType(code_action_kind=CodeActionClientCapabilitiesCodeActionLiteralSupportTypeCodeActionKindType(value_set=['', 'quickfix', 'refactor', 'refactor.extract', 'refactor.inline', 'refactor.rewrite', 'source', 'source.organizeImports'])), is_preferred_support=True, disabled_support=None, data_support=True, resolve_support=CodeActionClientCapabilitiesResolveSupportType(properties=['edit']), honors_change_annotations=None), code_lens=None, document_link=None, color_provider=None, formatting=DocumentFormattingClientCapabilities(dynamic_registration=True), range_formatting=DocumentRangeFormattingClientCapabilities(dynamic_registration=True, ranges_support=None), on_type_formatting=None, rename=RenameClientCapabilities(dynamic_registration=True, prepare_support=True, prepare_support_default_behavior=None, honors_change_annotations=None), folding_range=None, selection_range=None, publish_diagnostics=PublishDiagnosticsClientCapabilities(related_information=True, tag_support=PublishDiagnosticsClientCapabilitiesTagSupportType(value_set=[<DiagnosticTag.Unnecessary: 1>, <DiagnosticTag.Deprecated: 2>]), version_support=None, code_description_support=None, data_support=True), call_hierarchy=CallHierarchyClientCapabilities(dynamic_registration=False), semantic_tokens=SemanticTokensClientCapabilities(requests=SemanticTokensClientCapabilitiesRequestsType(range=False, full=SemanticTokensClientCapabilitiesRequestsTypeFullType1(delta=True)), token_types=['namespace', 'type', 'class', 'enum', 'interface', 'struct', 'typeParameter', 'parameter', 'variable', 'property', 'enumMember', 'event', 'function', 'method', 'macro', 'keyword', 'modifier', 'comment', 'string', 'number', 'regexp', 'operator', 'decorator'], token_modifiers=['declaration', 'definition', 'readonly', 'static', 'deprecated', 'abstract', 'async', 'modification', 'documentation', 'defaultLibrary'], formats=[<TokenFormat.Relative: 'relative'>], dynamic_registration=False, overlapping_token_support=True, multiline_token_support=False, server_cancel_support=False, augments_syntax_tokens=True), linked_editing_range=None, moniker=None, type_hierarchy=None, inline_value=None, inlay_hint=InlayHintClientCapabilities(dynamic_registration=True, resolve_support=InlayHintClientCapabilitiesResolveSupportType(properties=['textEdits', 'tooltip', 'location', 'command'])), diagnostic=DiagnosticClientCapabilities(dynamic_registration=False, related_document_support=None), inline_completion=None), notebook_document=None, window=WindowClientCapabilities(work_done_progress=True, show_message=ShowMessageRequestClientCapabilities(message_action_item=ShowMessageRequestClientCapabilitiesMessageActionItemType(additional_properties_support=False)), show_document=ShowDocumentClientCapabilities(support=True)), general=GeneralClientCapabilities(stale_request_support=None, regular_expressions=None, markdown=None, position_encodings=['utf-16']), experimental=None), process_id=975954, client_info=InitializeParamsClientInfoType(name='Neovim', version='0.10.4+v0.10.4'), locale=None, root_path='/home/elmortti/.config/nvim', root_uri='file:///home/elmortti/.config/nvim', initialization_options={'model_name': 'deepseek-coder:base', 'ollama_model_opts': {'num_predict': 40, 'temperature': 0.1}, 'stream_suggestion': False}, trace=<TraceValues.Off: 'off'>, work_done_token='1', workspace_folders=[WorkspaceFolder(uri='file:///home/elmortti/.config/nvim', name='/home/elmortti/.config/nvim')])
2025-02-02 06:17:37,858 - pygls.protocol.language_server - DEBUG - Server capabilities: {"positionEncoding": "utf-16", "textDocumentSync": {"openClose": true, "change": 2, "willSave": false, "willSaveWaitUntil": false, "save": false}, "completionProvider": {}, "executeCommandProvider": {"commands": []}, "workspace": {"workspaceFolders": {"supported": true, "changeNotifications": true}, "fileOperations": {}}}
2025-02-02 06:17:37,858 - ollama_copilot - INFO - Initializing Ollama LSP server
2025-02-02 06:17:37,861 - ollama_copilot - INFO - Initialized with model: deepseek-coder:base
2025-02-02 06:17:37,864 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 1, "jsonrpc": "2.0", "result": {"capabilities": {"positionEncoding": "utf-16", "textDocumentSync": {"openClose": true, "change": 2, "willSave": false, "willSaveWaitUntil": false, "save": false}, "completionProvider": {}, "executeCommandProvider": {"commands": []}, "workspace": {"workspaceFolders": {"supported": true, "changeNotifications": true}, "fileOperations": {}}}, "serverInfo": {"name": "ollama-server", "version": "v0.3"}}}
2025-02-02 06:17:37,864 - pygls.server - DEBUG - Content length: 52
2025-02-02 06:17:37,864 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 52\r\n\r\n{"method":"initialized","jsonrpc":"2.0","params":{}}'
2025-02-02 06:17:37,864 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:37,864 - pygls.server - DEBUG - Content length: 678
2025-02-02 06:17:37,865 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 678\r\n\r\n{"method":"textDocument\\/didOpen","jsonrpc":"2.0","params":{"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","languageId":"lua","text":"-- Custom configuration (defaults shown)\\nreturn {\\n\\t\'jasper-clarke\\/collama.nvim\',\\n\\topts = {\\n\\t\\tmodel_name = \\"deepseek-coder:1.3b\\",\\n\\t\\tstream_suggestion = true,\\n\\t\\tpython_command = \\"python3\\",\\n\\t\\tfiletypes = { \'python\', \'lua\', \'vim\', \\"markdown\\" },\\n\\t\\tollama_model_opts = {\\n\\t\\t\\tnum_predict = 40,\\n\\t\\t\\ttemperature = 0.1,\\n\\t\\t},\\n\\t\\tkeymaps = {\\n\\t\\t\\tsuggestion = \'<leader>os\',\\n\\t\\t\\treject = \'<leader>or\',\\n\\t\\t\\tinsert_accept = \'<Tab>\',\\n\\n\\t\\t},\\n\\t}\\n}\\n","version":0}}}'
2025-02-02 06:17:37,865 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:39,777 - pygls.server - DEBUG - Content length: 397
2025-02-02 06:17:39,777 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 397\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"","range":{"start":{"character":27,"line":15},"end":{"character":27,"line":15}}},{"rangeLength":1,"text":"","range":{"start":{"character":0,"line":16},"end":{"character":0,"line":17}}}],"textDocument":{"version":4,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:39,779 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:39,779 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:42,030 - pygls.server - DEBUG - Content length: 202
2025-02-02 06:17:42,031 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 202\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":2,"params":{"position":{"character":26,"line":15},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:42,033 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:42,033 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 2, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:43,560 - pygls.server - DEBUG - Content length: 301
2025-02-02 06:17:43,560 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 301\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"\\n\\t\\t\\t","range":{"start":{"character":27,"line":15},"end":{"character":27,"line":15}}}],"textDocument":{"version":5,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:43,560 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:43,561 - ollama_copilot - DEBUG - Document change detected: 
			
2025-02-02 06:17:44,313 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:44,313 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":" ","range":{"start":{"character":3,"line":16},"end":{"character":3,"line":16}}}],"textDocument":{"version":6,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:44,313 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:44,314 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:17:44,314 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:44,314 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:17:44,406 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:44,407 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"o","range":{"start":{"character":4,"line":16},"end":{"character":4,"line":16}}}],"textDocument":{"version":7,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:44,407 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:44,408 - ollama_copilot - DEBUG - Document change detected: o
2025-02-02 06:17:44,408 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:44,408 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:17:44,409 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:17:44,409 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":3,"params":{"position":{"character":5,"line":16},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:44,410 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:44,411 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 3, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:44,506 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:44,506 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"s","range":{"start":{"character":5,"line":16},"end":{"character":5,"line":16}}}],"textDocument":{"version":8,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:44,506 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:44,506 - ollama_copilot - DEBUG - Document change detected: s
2025-02-02 06:17:44,507 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:44,507 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:17:45,007 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:17:45,008 - ollama_copilot - DEBUG - Position - Line: 16, Character: 6
2025-02-02 06:17:45,008 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
2025-02-02 06:17:45,010 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7859f472d2b0>
2025-02-02 06:17:45,010 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:17:45,010 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:17:45,010 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:17:45,010 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:17:45,010 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:17:46,004 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:17:46 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:17:46,004 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:17:46,004 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:17:46,005 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:17:46,005 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:17:46,005 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:17:46,005 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 6, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:17:46,005 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 6, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:17:46,005 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:17:47,261 - pygls.server - DEBUG - Content length: 291
2025-02-02 06:17:47,262 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 291\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":5,"line":16},"end":{"character":6,"line":16}}}],"textDocument":{"version":9,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:47,262 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:47,262 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:47,490 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:47,490 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":4,"line":16},"end":{"character":5,"line":16}}}],"textDocument":{"version":10,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:47,491 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:47,491 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:47,775 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:47,776 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":3,"line":16},"end":{"character":4,"line":16}}}],"textDocument":{"version":11,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:47,776 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:47,777 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:53,883 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:53,883 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":" ","range":{"start":{"character":3,"line":16},"end":{"character":3,"line":16}}}],"textDocument":{"version":12,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:53,884 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:53,884 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:17:53,884 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:53,884 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:17:53,997 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:53,997 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"0","range":{"start":{"character":4,"line":16},"end":{"character":4,"line":16}}}],"textDocument":{"version":13,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:53,998 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:53,998 - ollama_copilot - DEBUG - Document change detected: 0
2025-02-02 06:17:53,998 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:53,998 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:17:53,998 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:17:53,999 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":4,"params":{"position":{"character":5,"line":16},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:53,999 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:53,999 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 4, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:54,086 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:54,087 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"s","range":{"start":{"character":5,"line":16},"end":{"character":5,"line":16}}}],"textDocument":{"version":14,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:54,087 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:54,087 - ollama_copilot - DEBUG - Document change detected: s
2025-02-02 06:17:54,087 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:54,087 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:17:54,088 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:17:54,088 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":5,"params":{"position":{"character":6,"line":16},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:54,088 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:54,088 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 5, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:54,540 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:54,540 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":5,"line":16},"end":{"character":6,"line":16}}}],"textDocument":{"version":15,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:54,540 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:54,540 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:54,544 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:17:54,544 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":6,"params":{"position":{"character":5,"line":16},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:54,544 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:54,544 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 6, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:54,589 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:17:54,589 - ollama_copilot - DEBUG - Position - Line: 16, Character: 6
2025-02-02 06:17:54,590 - httpcore.connection - DEBUG - close.started
2025-02-02 06:17:54,590 - httpcore.connection - DEBUG - close.complete
2025-02-02 06:17:54,590 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
2025-02-02 06:17:54,591 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7859f47360d0>
2025-02-02 06:17:54,591 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:17:54,591 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:17:54,591 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:17:54,592 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:17:54,592 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:17:54,619 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:17:54 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:17:54,620 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:17:54,620 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:17:54,620 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:17:54,620 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:17:54,620 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:17:54,620 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 6, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:17:54,620 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 6, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:17:54,620 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:17:54,723 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:54,724 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":4,"line":16},"end":{"character":5,"line":16}}}],"textDocument":{"version":16,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:54,724 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:54,724 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:54,924 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:54,924 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":3,"line":16},"end":{"character":4,"line":16}}}],"textDocument":{"version":17,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:54,925 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:54,925 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:55,725 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:55,725 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":" ","range":{"start":{"character":3,"line":16},"end":{"character":3,"line":16}}}],"textDocument":{"version":18,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:55,725 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:55,726 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:17:55,726 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:55,726 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:17:55,842 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:55,843 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"o","range":{"start":{"character":4,"line":16},"end":{"character":4,"line":16}}}],"textDocument":{"version":19,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:55,843 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:55,843 - ollama_copilot - DEBUG - Document change detected: o
2025-02-02 06:17:55,843 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:55,843 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:17:55,843 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:17:55,843 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":7,"params":{"position":{"character":5,"line":16},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:55,843 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:55,843 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 7, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:55,975 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:55,975 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"s","range":{"start":{"character":5,"line":16},"end":{"character":5,"line":16}}}],"textDocument":{"version":20,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:55,975 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:55,975 - ollama_copilot - DEBUG - Document change detected: s
2025-02-02 06:17:55,975 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:55,976 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:17:56,410 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:56,410 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":5,"line":16},"end":{"character":6,"line":16}}}],"textDocument":{"version":21,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:56,410 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:56,411 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:56,476 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:17:56,476 - ollama_copilot - DEBUG - Position - Line: 16, Character: 6
2025-02-02 06:17:56,477 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:17:56,477 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:17:56,477 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:17:56,477 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:17:56,477 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:17:56,499 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:17:56 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:17:56,499 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:17:56,499 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:17:56,499 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:17:56,499 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:17:56,499 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:17:56,499 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 6, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:17:56,499 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 6, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:17:56,500 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:17:56,576 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:56,576 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":4,"line":16},"end":{"character":5,"line":16}}}],"textDocument":{"version":22,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:56,576 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:56,576 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:56,985 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:17:56,986 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":3,"line":16},"end":{"character":4,"line":16}}}],"textDocument":{"version":23,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:56,986 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:56,987 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:17:58,593 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:58,594 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"i","range":{"start":{"character":3,"line":16},"end":{"character":3,"line":16}}}],"textDocument":{"version":24,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:58,594 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:58,595 - ollama_copilot - DEBUG - Document change detected: i
2025-02-02 06:17:58,595 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:58,595 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:17:58,603 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:17:58,603 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":8,"params":{"position":{"character":4,"line":16},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:58,603 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:17:58,603 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 8, "jsonrpc": "2.0", "result": []}
2025-02-02 06:17:59,097 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:17:59,097 - ollama_copilot - DEBUG - Position - Line: 16, Character: 4
2025-02-02 06:17:59,098 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:17:59,098 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:17:59,098 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:17:59,098 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:17:59,099 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:17:59,126 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:17:59 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:17:59,126 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:17:59,126 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:17:59,126 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:17:59,126 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:17:59,127 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:17:59,127 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 4, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:17:59,127 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 4, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:17:59,127 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:17:59,187 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:59,188 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"n","range":{"start":{"character":4,"line":16},"end":{"character":4,"line":16}}}],"textDocument":{"version":25,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:59,188 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:59,188 - ollama_copilot - DEBUG - Document change detected: n
2025-02-02 06:17:59,188 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:59,188 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:17:59,689 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:17:59,689 - ollama_copilot - DEBUG - Position - Line: 16, Character: 5
2025-02-02 06:17:59,690 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:17:59,690 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:17:59,690 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:17:59,690 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:17:59,690 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:17:59,712 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:17:59 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:17:59,712 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:17:59,712 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:17:59,712 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:17:59,712 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:17:59,712 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:17:59,712 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 5, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:17:59,712 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 5, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:17:59,712 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:17:59,798 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:17:59,799 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"e","range":{"start":{"character":5,"line":16},"end":{"character":5,"line":16}}}],"textDocument":{"version":26,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:17:59,799 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:17:59,799 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:17:59,799 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:17:59,799 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:00,300 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:18:00,300 - ollama_copilot - DEBUG - Position - Line: 16, Character: 6
2025-02-02 06:18:00,301 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:18:00,302 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:18:00,302 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:18:00,302 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:18:00,302 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:18:00,332 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:18:00 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:18:00,332 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:18:00,332 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:18:00,333 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:18:00,333 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:18:00,333 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:18:00,333 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 6, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:18:00,333 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 6, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:18:00,333 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:18:00,447 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:18:00,447 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"r","range":{"start":{"character":6,"line":16},"end":{"character":6,"line":16}}}],"textDocument":{"version":27,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:00,447 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:00,447 - ollama_copilot - DEBUG - Document change detected: r
2025-02-02 06:18:00,447 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:00,447 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:00,948 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:18:00,948 - ollama_copilot - DEBUG - Position - Line: 16, Character: 7
2025-02-02 06:18:00,949 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:18:00,950 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:18:00,950 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:18:00,950 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:18:00,950 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:18:00,979 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:18:00 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:18:00,979 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:18:00,979 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:18:00,979 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:18:00,979 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:18:00,979 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:18:00,979 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 7, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:18:00,980 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 7, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:18:00,980 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:18:01,010 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:18:01,010 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"t","range":{"start":{"character":7,"line":16},"end":{"character":7,"line":16}}}],"textDocument":{"version":28,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:01,011 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:01,011 - ollama_copilot - DEBUG - Document change detected: t
2025-02-02 06:18:01,011 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:01,011 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:01,511 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:18:01,512 - ollama_copilot - DEBUG - Position - Line: 16, Character: 8
2025-02-02 06:18:01,512 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:18:01,512 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:18:01,512 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:18:01,512 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:18:01,512 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:18:01,532 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:18:01 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:18:01,532 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:18:01,532 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:18:01,532 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:18:01,533 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:18:01,533 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:18:01,533 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 8, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:18:01,533 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 8, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:18:01,533 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:18:02,499 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:18:02,499 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"y","range":{"start":{"character":8,"line":16},"end":{"character":8,"line":16}}}],"textDocument":{"version":29,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:02,499 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:02,500 - ollama_copilot - DEBUG - Document change detected: y
2025-02-02 06:18:02,500 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:02,500 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:02,862 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:18:02,862 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"_","range":{"start":{"character":9,"line":16},"end":{"character":9,"line":16}}}],"textDocument":{"version":30,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:02,862 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:02,862 - ollama_copilot - DEBUG - Document change detected: _
2025-02-02 06:18:02,863 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:02,863 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:03,363 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:18:03,364 - ollama_copilot - DEBUG - Position - Line: 16, Character: 10
2025-02-02 06:18:03,364 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:18:03,364 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:18:03,364 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:18:03,364 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:18:03,364 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:18:03,384 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:18:03 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:18:03,384 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:18:03,384 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:18:03,384 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:18:03,384 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:18:03,384 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:18:03,384 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 10, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:18:03,384 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 10, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:18:03,384 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:18:03,810 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:03,811 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"d","range":{"start":{"character":10,"line":16},"end":{"character":10,"line":16}}}],"textDocument":{"version":31,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:03,811 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:03,812 - ollama_copilot - DEBUG - Document change detected: d
2025-02-02 06:18:03,812 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:03,812 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:03,960 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:03,961 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"e","range":{"start":{"character":11,"line":16},"end":{"character":11,"line":16}}}],"textDocument":{"version":32,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:03,961 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:03,961 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:18:03,961 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:03,961 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:04,462 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:18:04,462 - ollama_copilot - DEBUG - Position - Line: 16, Character: 12
2025-02-02 06:18:04,462 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:18:04,462 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:18:04,463 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:18:04,463 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:18:04,463 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:18:04,488 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:18:04 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:18:04,488 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:18:04,488 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:18:04,488 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:18:04,488 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:18:04,488 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:18:04,488 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 12, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:18:04,488 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 12, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:18:04,488 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:18:14,843 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:14,844 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"c","range":{"start":{"character":12,"line":16},"end":{"character":12,"line":16}}}],"textDocument":{"version":33,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:14,844 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:14,844 - ollama_copilot - DEBUG - Document change detected: c
2025-02-02 06:18:14,844 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:14,845 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:15,254 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:15,254 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"l","range":{"start":{"character":13,"line":16},"end":{"character":13,"line":16}}}],"textDocument":{"version":34,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:15,254 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:15,255 - ollama_copilot - DEBUG - Document change detected: l
2025-02-02 06:18:15,255 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:15,255 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:15,756 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:18:15,756 - ollama_copilot - DEBUG - Position - Line: 16, Character: 14
2025-02-02 06:18:15,757 - httpcore.connection - DEBUG - close.started
2025-02-02 06:18:15,758 - httpcore.connection - DEBUG - close.complete
2025-02-02 06:18:15,758 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
2025-02-02 06:18:15,759 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7859f4735590>
2025-02-02 06:18:15,759 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:18:15,759 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:18:15,759 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:18:15,759 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:18:15,759 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:18:15,788 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:18:15 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:18:15,788 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:18:15,788 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:18:15,788 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:18:15,788 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:18:15,788 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:18:15,788 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 14, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:18:15,788 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 14, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:18:15,788 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:18:54,740 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:18:54,741 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":13,"line":16},"end":{"character":14,"line":16}}}],"textDocument":{"version":35,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:54,741 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:54,741 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:18:54,751 - pygls.server - DEBUG - Content length: 230
2025-02-02 06:18:54,751 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 230\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":9,"params":{"position":{"character":13,"line":16},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:54,752 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:18:54,752 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 9, "jsonrpc": "2.0", "result": []}
2025-02-02 06:18:54,928 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:18:54,929 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":12,"line":16},"end":{"character":13,"line":16}}}],"textDocument":{"version":36,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:54,929 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:54,929 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:18:55,124 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:18:55,124 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":11,"line":16},"end":{"character":12,"line":16}}}],"textDocument":{"version":37,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:55,125 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:55,125 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:18:55,507 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:18:55,508 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":10,"line":16},"end":{"character":11,"line":16}}}],"textDocument":{"version":38,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:55,508 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:55,508 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:18:55,882 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:55,883 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"d","range":{"start":{"character":10,"line":16},"end":{"character":10,"line":16}}}],"textDocument":{"version":39,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:55,883 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:55,883 - ollama_copilot - DEBUG - Document change detected: d
2025-02-02 06:18:55,883 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:55,883 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:56,030 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:56,031 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"e","range":{"start":{"character":11,"line":16},"end":{"character":11,"line":16}}}],"textDocument":{"version":40,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:56,031 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:56,031 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:18:56,031 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:56,031 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:56,313 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:56,313 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"c","range":{"start":{"character":12,"line":16},"end":{"character":12,"line":16}}}],"textDocument":{"version":41,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:56,314 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:56,314 - ollama_copilot - DEBUG - Document change detected: c
2025-02-02 06:18:56,314 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:56,314 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:56,815 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:18:56,815 - ollama_copilot - DEBUG - Position - Line: 16, Character: 13
2025-02-02 06:18:56,816 - httpcore.connection - DEBUG - close.started
2025-02-02 06:18:56,817 - httpcore.connection - DEBUG - close.complete
2025-02-02 06:18:56,817 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
2025-02-02 06:18:56,818 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7859f34f8c30>
2025-02-02 06:18:56,818 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:18:56,818 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:18:56,818 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:18:56,818 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:18:56,818 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:18:56,850 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:18:56 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:18:56,850 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:18:56,850 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:18:56,850 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:18:56,850 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:18:56,850 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:18:56,850 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 13, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:18:56,850 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 13, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:18:56,850 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:18:56,956 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:56,956 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"l","range":{"start":{"character":13,"line":16},"end":{"character":13,"line":16}}}],"textDocument":{"version":42,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:56,956 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:56,956 - ollama_copilot - DEBUG - Document change detected: l
2025-02-02 06:18:56,956 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:56,956 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:57,246 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:57,246 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"i","range":{"start":{"character":14,"line":16},"end":{"character":14,"line":16}}}],"textDocument":{"version":43,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:57,246 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:57,246 - ollama_copilot - DEBUG - Document change detected: i
2025-02-02 06:18:57,246 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:57,246 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:57,598 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:57,598 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"n","range":{"start":{"character":15,"line":16},"end":{"character":15,"line":16}}}],"textDocument":{"version":44,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:57,598 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:57,598 - ollama_copilot - DEBUG - Document change detected: n
2025-02-02 06:18:57,598 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:57,598 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:58,099 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:18:58,099 - ollama_copilot - DEBUG - Position - Line: 16, Character: 16
2025-02-02 06:18:58,101 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:18:58,101 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:18:58,101 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:18:58,101 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:18:58,101 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:18:58,129 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:18:58 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:18:58,129 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:18:58,129 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:18:58,129 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:18:58,129 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:18:58,129 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:18:58,129 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 16, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:18:58,129 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 16, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:18:58,130 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:18:58,434 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:58,435 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"e","range":{"start":{"character":16,"line":16},"end":{"character":16,"line":16}}}],"textDocument":{"version":45,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:58,435 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:58,436 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:18:58,436 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:58,436 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:58,661 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:58,661 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":" ","range":{"start":{"character":17,"line":16},"end":{"character":17,"line":16}}}],"textDocument":{"version":46,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:58,661 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:58,661 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:18:58,662 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:58,662 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:59,074 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:59,074 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"O","range":{"start":{"character":18,"line":16},"end":{"character":18,"line":16}}}],"textDocument":{"version":47,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:59,074 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:59,074 - ollama_copilot - DEBUG - Document change detected: O
2025-02-02 06:18:59,075 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:59,075 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:59,077 - pygls.server - DEBUG - Content length: 231
2025-02-02 06:18:59,078 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 231\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":10,"params":{"position":{"character":19,"line":16},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:59,078 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:18:59,078 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 10, "jsonrpc": "2.0", "result": []}
2025-02-02 06:18:59,314 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:18:59,315 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":" ","range":{"start":{"character":19,"line":16},"end":{"character":19,"line":16}}}],"textDocument":{"version":48,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:59,316 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:59,316 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:18:59,316 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:18:59,316 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:18:59,721 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:18:59,721 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":19,"line":16},"end":{"character":20,"line":16}}}],"textDocument":{"version":49,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:59,721 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:59,721 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:18:59,723 - pygls.server - DEBUG - Content length: 231
2025-02-02 06:18:59,723 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 231\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":11,"params":{"position":{"character":19,"line":16},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:59,723 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:18:59,723 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 11, "jsonrpc": "2.0", "result": []}
2025-02-02 06:18:59,818 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:18:59,818 - ollama_copilot - DEBUG - Position - Line: 16, Character: 20
2025-02-02 06:18:59,818 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:18:59,818 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:18:59,818 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:18:59,818 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:18:59,818 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:18:59,838 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:18:59 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:18:59,838 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:18:59,839 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:18:59,839 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:18:59,839 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:18:59,839 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:18:59,839 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 20, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:18:59,839 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 20, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:18:59,839 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:18:59,875 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:18:59,875 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":18,"line":16},"end":{"character":19,"line":16}}}],"textDocument":{"version":50,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:18:59,875 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:18:59,875 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:19:00,197 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:19:00,197 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"=","range":{"start":{"character":18,"line":16},"end":{"character":18,"line":16}}}],"textDocument":{"version":51,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:00,198 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:00,198 - ollama_copilot - DEBUG - Document change detected: =
2025-02-02 06:19:00,198 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:00,198 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:00,304 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:19:00,304 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":" ","range":{"start":{"character":19,"line":16},"end":{"character":19,"line":16}}}],"textDocument":{"version":52,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:00,304 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:00,304 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:19:00,304 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:00,304 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:00,488 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:19:00,489 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"\'","range":{"start":{"character":20,"line":16},"end":{"character":20,"line":16}}}],"textDocument":{"version":53,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:00,489 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:00,489 - ollama_copilot - DEBUG - Document change detected: '
2025-02-02 06:19:00,489 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:00,489 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:00,990 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:19:00,990 - ollama_copilot - DEBUG - Position - Line: 16, Character: 21
2025-02-02 06:19:00,990 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:19:00,990 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:19:00,990 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:19:00,990 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:19:00,990 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:19:01,011 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:19:01 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:19:01,011 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:19:01,011 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:19:01,011 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:19:01,011 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:19:01,011 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:19:01,011 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 21, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:19:01,011 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 21, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:19:01,011 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:19:04,603 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:19:04,603 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"<","range":{"start":{"character":21,"line":16},"end":{"character":21,"line":16}}}],"textDocument":{"version":54,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:04,603 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:04,603 - ollama_copilot - DEBUG - Document change detected: <
2025-02-02 06:19:04,603 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:04,603 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:05,104 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:19:05,104 - ollama_copilot - DEBUG - Position - Line: 16, Character: 22
2025-02-02 06:19:05,105 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:19:05,106 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:19:05,106 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:19:05,106 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:19:05,106 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:19:05,137 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:19:05 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:19:05,137 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:19:05,137 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:19:05,137 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:19:05,137 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:19:05,137 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:19:05,137 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 17, 'character': 22, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:19:05,138 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 17, "character": 22, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:19:05,138 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:19:10,148 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:10,148 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":23,"text":"","range":{"start":{"character":0,"line":16},"end":{"character":0,"line":17}}}],"textDocument":{"version":55,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:10,148 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:10,148 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:19:12,182 - pygls.server - DEBUG - Content length: 303
2025-02-02 06:19:12,182 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 303\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"\\n\\t\\t\\t\\n","range":{"start":{"character":25,"line":14},"end":{"character":0,"line":15}}}],"textDocument":{"version":56,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:12,182 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:12,182 - ollama_copilot - DEBUG - Document change detected: 
			

2025-02-02 06:19:14,036 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:14,037 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"a","range":{"start":{"character":3,"line":15},"end":{"character":3,"line":15}}}],"textDocument":{"version":57,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:14,037 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:14,037 - ollama_copilot - DEBUG - Document change detected: a
2025-02-02 06:19:14,037 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:14,037 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:14,043 - pygls.server - DEBUG - Content length: 230
2025-02-02 06:19:14,043 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 230\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":12,"params":{"position":{"character":4,"line":15},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:14,043 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:19:14,044 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 12, "jsonrpc": "2.0", "result": []}
2025-02-02 06:19:14,538 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:19:14,539 - ollama_copilot - DEBUG - Position - Line: 15, Character: 4
2025-02-02 06:19:14,539 - httpcore.connection - DEBUG - close.started
2025-02-02 06:19:14,539 - httpcore.connection - DEBUG - close.complete
2025-02-02 06:19:14,539 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
2025-02-02 06:19:14,540 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7859f34fa060>
2025-02-02 06:19:14,540 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:19:14,540 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:19:14,540 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:19:14,540 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:19:14,540 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:19:14,573 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:19:14 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:19:14,574 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:19:14,574 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:19:14,574 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:19:14,574 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:19:14,574 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:19:14,574 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 16, 'character': 4, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:19:14,574 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 16, "character": 4, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:19:14,574 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:19:14,646 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:14,647 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"c","range":{"start":{"character":4,"line":15},"end":{"character":4,"line":15}}}],"textDocument":{"version":58,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:14,647 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:14,648 - ollama_copilot - DEBUG - Document change detected: c
2025-02-02 06:19:14,648 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:14,648 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:14,835 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:14,836 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"c","range":{"start":{"character":5,"line":15},"end":{"character":5,"line":15}}}],"textDocument":{"version":59,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:14,836 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:14,836 - ollama_copilot - DEBUG - Document change detected: c
2025-02-02 06:19:14,837 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:14,837 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:15,179 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:15,180 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"e","range":{"start":{"character":6,"line":15},"end":{"character":6,"line":15}}}],"textDocument":{"version":60,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:15,180 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:15,181 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:19:15,181 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:15,181 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:15,682 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:19:15,682 - ollama_copilot - DEBUG - Position - Line: 15, Character: 7
2025-02-02 06:19:15,682 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:19:15,682 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:19:15,682 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:19:15,682 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:19:15,682 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:19:15,705 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:19:15 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:19:15,705 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:19:15,705 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:19:15,706 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:19:15,706 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:19:15,706 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:19:15,706 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 16, 'character': 7, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:19:15,706 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 16, "character": 7, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:19:15,706 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:19:16,625 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:16,626 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"o","range":{"start":{"character":7,"line":15},"end":{"character":7,"line":15}}}],"textDocument":{"version":61,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:16,626 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:16,626 - ollama_copilot - DEBUG - Document change detected: o
2025-02-02 06:19:16,626 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:16,626 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:17,127 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:19:17,127 - ollama_copilot - DEBUG - Position - Line: 15, Character: 8
2025-02-02 06:19:17,127 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:19:17,127 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:19:17,127 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:19:17,127 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:19:17,127 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:19:17,152 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:19:17 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:19:17,152 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:19:17,152 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:19:17,153 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:19:17,153 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:19:17,153 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:19:17,153 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 16, 'character': 8, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:19:17,153 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 16, "character": 8, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:19:17,153 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:19:18,845 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:19:18,846 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":7,"line":15},"end":{"character":8,"line":15}}}],"textDocument":{"version":62,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:18,846 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:18,846 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:19:19,156 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:19,157 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"p","range":{"start":{"character":7,"line":15},"end":{"character":7,"line":15}}}],"textDocument":{"version":63,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:19,157 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:19,157 - ollama_copilot - DEBUG - Document change detected: p
2025-02-02 06:19:19,157 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:19,157 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:19,306 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:19,306 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"t","range":{"start":{"character":8,"line":15},"end":{"character":8,"line":15}}}],"textDocument":{"version":64,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:19,306 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:19,306 - ollama_copilot - DEBUG - Document change detected: t
2025-02-02 06:19:19,307 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:19,307 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:19,807 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:19:19,807 - ollama_copilot - DEBUG - Position - Line: 15, Character: 9
2025-02-02 06:19:19,808 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:19:19,808 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:19:19,808 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:19:19,808 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:19:19,809 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:19:19,836 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:19:19 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:19:19,837 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:19:19,837 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:19:19,837 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:19:19,837 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:19:19,837 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:19:19,837 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 16, 'character': 9, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:19:19,837 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 16, "character": 9, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:19:19,837 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:19:31,118 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:31,118 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":" ","range":{"start":{"character":9,"line":15},"end":{"character":9,"line":15}}}],"textDocument":{"version":65,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:31,119 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:31,119 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:19:31,119 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:31,119 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:31,222 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:19:31,223 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"o","range":{"start":{"character":10,"line":15},"end":{"character":10,"line":15}}}],"textDocument":{"version":66,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:31,223 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:31,223 - ollama_copilot - DEBUG - Document change detected: o
2025-02-02 06:19:31,223 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:31,223 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:31,223 - pygls.server - DEBUG - Content length: 231
2025-02-02 06:19:31,223 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 231\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":13,"params":{"position":{"character":11,"line":15},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:31,223 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:19:31,223 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 13, "jsonrpc": "2.0", "result": []}
2025-02-02 06:19:31,385 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:19:31,385 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"d","range":{"start":{"character":11,"line":15},"end":{"character":11,"line":15}}}],"textDocument":{"version":67,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:31,385 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:31,385 - ollama_copilot - DEBUG - Document change detected: d
2025-02-02 06:19:31,385 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:31,385 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:31,759 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:19:31,760 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":11,"line":15},"end":{"character":12,"line":15}}}],"textDocument":{"version":68,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:31,761 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:31,761 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:19:31,887 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:19:31,887 - ollama_copilot - DEBUG - Position - Line: 15, Character: 12
2025-02-02 06:19:31,887 - httpcore.connection - DEBUG - close.started
2025-02-02 06:19:31,887 - httpcore.connection - DEBUG - close.complete
2025-02-02 06:19:31,887 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
2025-02-02 06:19:31,888 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7859f3500cb0>
2025-02-02 06:19:31,888 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:19:31,888 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:19:31,888 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:19:31,888 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:19:31,888 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:19:31,912 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:19:31 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:19:31,912 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:19:31,912 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:19:31,912 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:19:31,912 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:19:31,912 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:19:31,913 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 16, 'character': 12, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:19:31,913 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 16, "character": 12, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:19:31,913 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:19:31,977 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:19:31,977 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":10,"line":15},"end":{"character":11,"line":15}}}],"textDocument":{"version":69,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:31,978 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:31,978 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:19:32,174 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:32,175 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":9,"line":15},"end":{"character":10,"line":15}}}],"textDocument":{"version":70,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:32,175 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:32,175 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:19:32,181 - pygls.server - DEBUG - Content length: 230
2025-02-02 06:19:32,181 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 230\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":14,"params":{"position":{"character":9,"line":15},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:32,181 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:19:32,181 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 14, "jsonrpc": "2.0", "result": []}
2025-02-02 06:19:32,455 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:32,455 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":" ","range":{"start":{"character":9,"line":15},"end":{"character":9,"line":15}}}],"textDocument":{"version":71,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:32,455 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:32,456 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:19:32,456 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:32,456 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:32,602 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:19:32,602 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"o","range":{"start":{"character":10,"line":15},"end":{"character":10,"line":15}}}],"textDocument":{"version":72,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:32,603 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:32,603 - ollama_copilot - DEBUG - Document change detected: o
2025-02-02 06:19:32,603 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:32,603 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:32,605 - pygls.server - DEBUG - Content length: 231
2025-02-02 06:19:32,605 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 231\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":15,"params":{"position":{"character":11,"line":15},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:32,606 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:19:32,606 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 15, "jsonrpc": "2.0", "result": []}
2025-02-02 06:19:32,702 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:19:32,702 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"s","range":{"start":{"character":11,"line":15},"end":{"character":11,"line":15}}}],"textDocument":{"version":73,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:32,702 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:32,702 - ollama_copilot - DEBUG - Document change detected: s
2025-02-02 06:19:32,702 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:32,702 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:32,999 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:19:32,999 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":11,"line":15},"end":{"character":12,"line":15}}}],"textDocument":{"version":74,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:32,999 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:32,999 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:19:33,162 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:19:33,162 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":10,"line":15},"end":{"character":11,"line":15}}}],"textDocument":{"version":75,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:33,162 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:33,162 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:19:33,203 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:19:33,203 - ollama_copilot - DEBUG - Position - Line: 15, Character: 12
2025-02-02 06:19:33,204 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:19:33,204 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:19:33,204 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:19:33,204 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:19:33,204 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:19:33,226 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:19:33 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:19:33,226 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:19:33,226 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:19:33,226 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:19:33,226 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:19:33,226 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:19:33,226 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 16, 'character': 12, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:19:33,226 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 16, "character": 12, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:19:33,226 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:19:33,350 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:33,351 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":1,"text":"","range":{"start":{"character":9,"line":15},"end":{"character":10,"line":15}}}],"textDocument":{"version":76,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:33,351 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:33,351 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:19:33,354 - pygls.server - DEBUG - Content length: 230
2025-02-02 06:19:33,354 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 230\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":16,"params":{"position":{"character":9,"line":15},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:33,354 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:19:33,354 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 16, "jsonrpc": "2.0", "result": []}
2025-02-02 06:19:33,999 - pygls.server - DEBUG - Content length: 202
2025-02-02 06:19:33,999 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 202\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":17,"params":{"position":{"character":8,"line":15},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:33,999 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:19:33,999 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 17, "jsonrpc": "2.0", "result": []}
2025-02-02 06:19:38,005 - pygls.server - DEBUG - Content length: 202
2025-02-02 06:19:38,006 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 202\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":18,"params":{"position":{"character":8,"line":15},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:38,006 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:19:38,007 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 18, "jsonrpc": "2.0", "result": []}
2025-02-02 06:19:42,163 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:19:42,163 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":" ","range":{"start":{"character":9,"line":15},"end":{"character":9,"line":15}}}],"textDocument":{"version":77,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:42,163 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:42,163 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:19:42,163 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:42,164 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:42,664 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:19:42,664 - ollama_copilot - DEBUG - Position - Line: 15, Character: 10
2025-02-02 06:19:42,665 - httpcore.connection - DEBUG - close.started
2025-02-02 06:19:42,665 - httpcore.connection - DEBUG - close.complete
2025-02-02 06:19:42,665 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
2025-02-02 06:19:42,665 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7859f35056a0>
2025-02-02 06:19:42,665 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:19:42,665 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:19:42,666 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:19:42,666 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:19:42,666 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:19:42,687 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:19:42 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:19:42,687 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:19:42,687 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:19:42,687 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:19:42,687 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:19:42,687 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:19:42,687 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 16, 'character': 10, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:19:42,687 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 16, "character": 10, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:19:42,687 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:19:42,792 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:19:42,792 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"=","range":{"start":{"character":10,"line":15},"end":{"character":10,"line":15}}}],"textDocument":{"version":78,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:42,792 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:42,792 - ollama_copilot - DEBUG - Document change detected: =
2025-02-02 06:19:42,792 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:42,792 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:43,172 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:19:43,172 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":" ","range":{"start":{"character":11,"line":15},"end":{"character":11,"line":15}}}],"textDocument":{"version":79,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:19:43,173 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:19:43,173 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:19:43,173 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:19:43,173 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:19:43,673 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:19:43,674 - ollama_copilot - DEBUG - Position - Line: 15, Character: 12
2025-02-02 06:19:43,675 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:19:43,675 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:19:43,675 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:19:43,675 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:19:43,675 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:19:43,704 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:19:43 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:19:43,704 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:19:43,704 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:19:43,704 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:19:43,704 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:19:43,704 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:19:43,704 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 16, 'character': 12, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:19:43,704 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 16, "character": 12, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:19:43,704 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:20:50,253 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:20:50,253 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":13,"text":"","range":{"start":{"character":2,"line":2},"end":{"character":15,"line":2}}}],"textDocument":{"version":80,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:20:50,253 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:20:50,253 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:20:50,258 - pygls.server - DEBUG - Content length: 298
2025-02-02 06:20:50,258 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 298\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"Jacob411","range":{"start":{"character":2,"line":2},"end":{"character":2,"line":2}}}],"textDocument":{"version":81,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:20:50,258 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:20:50,258 - ollama_copilot - DEBUG - Document change detected: Jacob411
2025-02-02 06:20:53,481 - pygls.server - DEBUG - Content length: 291
2025-02-02 06:20:53,482 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 291\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":":","range":{"start":{"character":9,"line":2},"end":{"character":9,"line":2}}}],"textDocument":{"version":83,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:20:53,482 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:20:53,482 - ollama_copilot - DEBUG - Document change detected: :
2025-02-02 06:20:53,771 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:20:53,771 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":0,"text":"S","range":{"start":{"character":10,"line":2},"end":{"character":10,"line":2}}}],"textDocument":{"version":84,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:20:53,772 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:20:53,772 - ollama_copilot - DEBUG - Document change detected: S
2025-02-02 06:20:53,784 - pygls.server - DEBUG - Content length: 230
2025-02-02 06:20:53,784 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 230\r\n\r\n{"method":"textDocument\\/completion","jsonrpc":"2.0","id":19,"params":{"position":{"character":11,"line":2},"context":{"triggerKind":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:20:53,784 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:20:53,784 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 19, "jsonrpc": "2.0", "result": []}
2025-02-02 06:20:57,345 - pygls.server - DEBUG - Content length: 291
2025-02-02 06:20:57,345 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 291\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"rangeLength":2,"text":"","range":{"start":{"character":9,"line":2},"end":{"character":11,"line":2}}}],"textDocument":{"version":85,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:20:57,345 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:20:57,346 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:21:05,189 - pygls.server - DEBUG - Content length: 45
2025-02-02 06:21:05,189 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 45\r\n\r\n{"method":"shutdown","jsonrpc":"2.0","id":20}'
2025-02-02 06:21:05,191 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:21:05,191 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 20, "jsonrpc": "2.0", "result": null}
2025-02-02 06:21:05,199 - pygls.server - DEBUG - Content length: 33
2025-02-02 06:21:05,200 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 33\r\n\r\n{"method":"exit","jsonrpc":"2.0"}'
2025-02-02 06:21:05,200 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:21:05,200 - pygls.server - INFO - Shutting down the server
2025-02-02 06:21:05,201 - pygls.server - INFO - Closing the event loop.
2025-02-02 06:22:45,665 - asyncio - DEBUG - Using selector: EpollSelector
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature exit
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature initialize
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature initialized
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature notebookDocument/didChange
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature notebookDocument/didClose
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature notebookDocument/didOpen
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature $/setTrace
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature shutdown
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature textDocument/didChange
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature textDocument/didClose
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature textDocument/didOpen
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature window/workDoneProgress/cancel
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature workspace/didChangeWorkspaceFolders
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered builtin feature workspace/executeCommand
2025-02-02 06:22:45,673 - pygls.feature_manager - INFO - Registered "initialize" with options "None"
2025-02-02 06:22:45,674 - pygls.feature_manager - INFO - Registered "textDocument/completion" with options "None"
2025-02-02 06:22:45,674 - pygls.feature_manager - INFO - Registered "textDocument/didChange" with options "None"
2025-02-02 06:22:45,674 - ollama_copilot - INFO - Starting Ollama LSP server
2025-02-02 06:22:45,674 - pygls.server - INFO - Starting IO server
2025-02-02 06:22:45,674 - pygls.server - DEBUG - Content length: 4170
2025-02-02 06:22:45,674 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 4170\r\n\r\n{"id":1,"method":"initialize","jsonrpc":"2.0","params":{"workspaceFolders":[{"name":"\\/home\\/elmortti\\/.config\\/nvim","uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim"}],"trace":"off","workDoneToken":"1","capabilities":{"window":{"workDoneProgress":true,"showMessage":{"messageActionItem":{"additionalPropertiesSupport":false}},"showDocument":{"support":true}},"textDocument":{"synchronization":{"willSaveWaitUntil":true,"willSave":true,"dynamicRegistration":false,"didSave":true},"codeAction":{"isPreferredSupport":true,"dataSupport":true,"codeActionLiteralSupport":{"codeActionKind":{"valueSet":["","quickfix","refactor","refactor.extract","refactor.inline","refactor.rewrite","source","source.organizeImports"]}},"dynamicRegistration":true,"resolveSupport":{"properties":["edit"]}},"hover":{"dynamicRegistration":true,"contentFormat":["markdown","plaintext"]},"rename":{"dynamicRegistration":true,"prepareSupport":true},"implementation":{"linkSupport":true},"typeDefinition":{"linkSupport":true},"diagnostic":{"dynamicRegistration":false},"definition":{"linkSupport":true,"dynamicRegistration":true},"signatureHelp":{"dynamicRegistration":false,"signatureInformation":{"documentationFormat":["markdown","plaintext"],"activeParameterSupport":true,"parameterInformation":{"labelOffsetSupport":true}}},"references":{"dynamicRegistration":false},"documentHighlight":{"dynamicRegistration":false},"documentSymbol":{"symbolKind":{"valueSet":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]},"hierarchicalDocumentSymbolSupport":true,"dynamicRegistration":false},"semanticTokens":{"serverCancelSupport":false,"augmentsSyntaxTokens":true,"dynamicRegistration":false,"formats":["relative"],"requests":{"full":{"delta":true},"range":false},"tokenTypes":["namespace","type","class","enum","interface","struct","typeParameter","parameter","variable","property","enumMember","event","function","method","macro","keyword","modifier","comment","string","number","regexp","operator","decorator"],"tokenModifiers":["declaration","definition","readonly","static","deprecated","abstract","async","modification","documentation","defaultLibrary"],"overlappingTokenSupport":true,"multilineTokenSupport":false},"completion":{"insertTextMode":1,"contextSupport":true,"completionList":{"itemDefaults":["commitCharacters","editRange","insertTextFormat","insertTextMode","data"]},"completionItemKind":{"valueSet":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]},"dynamicRegistration":false,"completionItem":{"snippetSupport":true,"commitCharactersSupport":true,"preselectSupport":true,"deprecatedSupport":true,"documentationFormat":["markdown","plaintext"],"resolveSupport":{"properties":["documentation","additionalTextEdits","insertTextFormat","insertTextMode","command"]},"labelDetailsSupport":true,"tagSupport":{"valueSet":[1]},"insertTextModeSupport":{"valueSet":[1,2]},"insertReplaceSupport":true}},"declaration":{"linkSupport":true},"publishDiagnostics":{"relatedInformation":true,"tagSupport":{"valueSet":[1,2]},"dataSupport":true},"formatting":{"dynamicRegistration":true},"rangeFormatting":{"dynamicRegistration":true},"callHierarchy":{"dynamicRegistration":false},"inlayHint":{"dynamicRegistration":true,"resolveSupport":{"properties":["textEdits","tooltip","location","command"]}}},"workspace":{"configuration":true,"didChangeConfiguration":{"dynamicRegistration":false},"workspaceFolders":true,"applyEdit":true,"workspaceEdit":{"resourceOperations":["rename","create","delete"]},"inlayHint":{"refreshSupport":true},"semanticTokens":{"refreshSupport":true},"symbol":{"dynamicRegistration":false,"symbolKind":{"valueSet":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]}},"didChangeWatchedFiles":{"dynamicRegistration":false,"relativePatternSupport":true}},"general":{"positionEncodings":["utf-16"]}},"rootUri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim","rootPath":"\\/home\\/elmortti\\/.config\\/nvim","clientInfo":{"name":"Neovim","version":"0.10.4+v0.10.4"},"initializationOptions":{"stream_suggestion":false,"ollama_model_opts":{"num_predict":40,"temperature":0.1},"model_name":"deepseek-coder:base"},"processId":986970}}'
2025-02-02 06:22:45,690 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:22:45,690 - pygls.protocol.language_server - INFO - Language server initialized InitializeParams(capabilities=ClientCapabilities(workspace=WorkspaceClientCapabilities(apply_edit=True, workspace_edit=WorkspaceEditClientCapabilities(document_changes=None, resource_operations=[<ResourceOperationKind.Rename: 'rename'>, <ResourceOperationKind.Create: 'create'>, <ResourceOperationKind.Delete: 'delete'>], failure_handling=None, normalizes_line_endings=None, change_annotation_support=None), did_change_configuration=DidChangeConfigurationClientCapabilities(dynamic_registration=False), did_change_watched_files=DidChangeWatchedFilesClientCapabilities(dynamic_registration=False, relative_pattern_support=True), symbol=WorkspaceSymbolClientCapabilities(dynamic_registration=False, symbol_kind=WorkspaceSymbolClientCapabilitiesSymbolKindType(value_set=[<SymbolKind.File: 1>, <SymbolKind.Module: 2>, <SymbolKind.Namespace: 3>, <SymbolKind.Package: 4>, <SymbolKind.Class: 5>, <SymbolKind.Method: 6>, <SymbolKind.Property: 7>, <SymbolKind.Field: 8>, <SymbolKind.Constructor: 9>, <SymbolKind.Enum: 10>, <SymbolKind.Interface: 11>, <SymbolKind.Function: 12>, <SymbolKind.Variable: 13>, <SymbolKind.Constant: 14>, <SymbolKind.String: 15>, <SymbolKind.Number: 16>, <SymbolKind.Boolean: 17>, <SymbolKind.Array: 18>, <SymbolKind.Object: 19>, <SymbolKind.Key: 20>, <SymbolKind.Null: 21>, <SymbolKind.EnumMember: 22>, <SymbolKind.Struct: 23>, <SymbolKind.Event: 24>, <SymbolKind.Operator: 25>, <SymbolKind.TypeParameter: 26>]), tag_support=None, resolve_support=None), execute_command=None, workspace_folders=True, configuration=True, semantic_tokens=SemanticTokensWorkspaceClientCapabilities(refresh_support=True), code_lens=None, file_operations=None, inline_value=None, inlay_hint=InlayHintWorkspaceClientCapabilities(refresh_support=True), diagnostics=None, folding_range=None), text_document=TextDocumentClientCapabilities(synchronization=TextDocumentSyncClientCapabilities(dynamic_registration=False, will_save=True, will_save_wait_until=True, did_save=True), completion=CompletionClientCapabilities(dynamic_registration=False, completion_item=CompletionClientCapabilitiesCompletionItemType(snippet_support=True, commit_characters_support=True, documentation_format=[<MarkupKind.Markdown: 'markdown'>, <MarkupKind.PlainText: 'plaintext'>], deprecated_support=True, preselect_support=True, tag_support=CompletionClientCapabilitiesCompletionItemTypeTagSupportType(value_set=[<CompletionItemTag.Deprecated: 1>]), insert_replace_support=True, resolve_support=CompletionClientCapabilitiesCompletionItemTypeResolveSupportType(properties=['documentation', 'additionalTextEdits', 'insertTextFormat', 'insertTextMode', 'command']), insert_text_mode_support=CompletionClientCapabilitiesCompletionItemTypeInsertTextModeSupportType(value_set=[<InsertTextMode.AsIs: 1>, <InsertTextMode.AdjustIndentation: 2>]), label_details_support=True), completion_item_kind=CompletionClientCapabilitiesCompletionItemKindType(value_set=[<CompletionItemKind.Text: 1>, <CompletionItemKind.Method: 2>, <CompletionItemKind.Function: 3>, <CompletionItemKind.Constructor: 4>, <CompletionItemKind.Field: 5>, <CompletionItemKind.Variable: 6>, <CompletionItemKind.Class: 7>, <CompletionItemKind.Interface: 8>, <CompletionItemKind.Module: 9>, <CompletionItemKind.Property: 10>, <CompletionItemKind.Unit: 11>, <CompletionItemKind.Value: 12>, <CompletionItemKind.Enum: 13>, <CompletionItemKind.Keyword: 14>, <CompletionItemKind.Snippet: 15>, <CompletionItemKind.Color: 16>, <CompletionItemKind.File: 17>, <CompletionItemKind.Reference: 18>, <CompletionItemKind.Folder: 19>, <CompletionItemKind.EnumMember: 20>, <CompletionItemKind.Constant: 21>, <CompletionItemKind.Struct: 22>, <CompletionItemKind.Event: 23>, <CompletionItemKind.Operator: 24>, <CompletionItemKind.TypeParameter: 25>]), insert_text_mode=<InsertTextMode.AsIs: 1>, context_support=True, completion_list=CompletionClientCapabilitiesCompletionListType(item_defaults=['commitCharacters', 'editRange', 'insertTextFormat', 'insertTextMode', 'data'])), hover=HoverClientCapabilities(dynamic_registration=True, content_format=[<MarkupKind.Markdown: 'markdown'>, <MarkupKind.PlainText: 'plaintext'>]), signature_help=SignatureHelpClientCapabilities(dynamic_registration=False, signature_information=SignatureHelpClientCapabilitiesSignatureInformationType(documentation_format=[<MarkupKind.Markdown: 'markdown'>, <MarkupKind.PlainText: 'plaintext'>], parameter_information=SignatureHelpClientCapabilitiesSignatureInformationTypeParameterInformationType(label_offset_support=True), active_parameter_support=True), context_support=None), declaration=DeclarationClientCapabilities(dynamic_registration=None, link_support=True), definition=DefinitionClientCapabilities(dynamic_registration=True, link_support=True), type_definition=TypeDefinitionClientCapabilities(dynamic_registration=None, link_support=True), implementation=ImplementationClientCapabilities(dynamic_registration=None, link_support=True), references=ReferenceClientCapabilities(dynamic_registration=False), document_highlight=DocumentHighlightClientCapabilities(dynamic_registration=False), document_symbol=DocumentSymbolClientCapabilities(dynamic_registration=False, symbol_kind=DocumentSymbolClientCapabilitiesSymbolKindType(value_set=[<SymbolKind.File: 1>, <SymbolKind.Module: 2>, <SymbolKind.Namespace: 3>, <SymbolKind.Package: 4>, <SymbolKind.Class: 5>, <SymbolKind.Method: 6>, <SymbolKind.Property: 7>, <SymbolKind.Field: 8>, <SymbolKind.Constructor: 9>, <SymbolKind.Enum: 10>, <SymbolKind.Interface: 11>, <SymbolKind.Function: 12>, <SymbolKind.Variable: 13>, <SymbolKind.Constant: 14>, <SymbolKind.String: 15>, <SymbolKind.Number: 16>, <SymbolKind.Boolean: 17>, <SymbolKind.Array: 18>, <SymbolKind.Object: 19>, <SymbolKind.Key: 20>, <SymbolKind.Null: 21>, <SymbolKind.EnumMember: 22>, <SymbolKind.Struct: 23>, <SymbolKind.Event: 24>, <SymbolKind.Operator: 25>, <SymbolKind.TypeParameter: 26>]), hierarchical_document_symbol_support=True, tag_support=None, label_support=None), code_action=CodeActionClientCapabilities(dynamic_registration=True, code_action_literal_support=CodeActionClientCapabilitiesCodeActionLiteralSupportType(code_action_kind=CodeActionClientCapabilitiesCodeActionLiteralSupportTypeCodeActionKindType(value_set=['', 'quickfix', 'refactor', 'refactor.extract', 'refactor.inline', 'refactor.rewrite', 'source', 'source.organizeImports'])), is_preferred_support=True, disabled_support=None, data_support=True, resolve_support=CodeActionClientCapabilitiesResolveSupportType(properties=['edit']), honors_change_annotations=None), code_lens=None, document_link=None, color_provider=None, formatting=DocumentFormattingClientCapabilities(dynamic_registration=True), range_formatting=DocumentRangeFormattingClientCapabilities(dynamic_registration=True, ranges_support=None), on_type_formatting=None, rename=RenameClientCapabilities(dynamic_registration=True, prepare_support=True, prepare_support_default_behavior=None, honors_change_annotations=None), folding_range=None, selection_range=None, publish_diagnostics=PublishDiagnosticsClientCapabilities(related_information=True, tag_support=PublishDiagnosticsClientCapabilitiesTagSupportType(value_set=[<DiagnosticTag.Unnecessary: 1>, <DiagnosticTag.Deprecated: 2>]), version_support=None, code_description_support=None, data_support=True), call_hierarchy=CallHierarchyClientCapabilities(dynamic_registration=False), semantic_tokens=SemanticTokensClientCapabilities(requests=SemanticTokensClientCapabilitiesRequestsType(range=False, full=SemanticTokensClientCapabilitiesRequestsTypeFullType1(delta=True)), token_types=['namespace', 'type', 'class', 'enum', 'interface', 'struct', 'typeParameter', 'parameter', 'variable', 'property', 'enumMember', 'event', 'function', 'method', 'macro', 'keyword', 'modifier', 'comment', 'string', 'number', 'regexp', 'operator', 'decorator'], token_modifiers=['declaration', 'definition', 'readonly', 'static', 'deprecated', 'abstract', 'async', 'modification', 'documentation', 'defaultLibrary'], formats=[<TokenFormat.Relative: 'relative'>], dynamic_registration=False, overlapping_token_support=True, multiline_token_support=False, server_cancel_support=False, augments_syntax_tokens=True), linked_editing_range=None, moniker=None, type_hierarchy=None, inline_value=None, inlay_hint=InlayHintClientCapabilities(dynamic_registration=True, resolve_support=InlayHintClientCapabilitiesResolveSupportType(properties=['textEdits', 'tooltip', 'location', 'command'])), diagnostic=DiagnosticClientCapabilities(dynamic_registration=False, related_document_support=None), inline_completion=None), notebook_document=None, window=WindowClientCapabilities(work_done_progress=True, show_message=ShowMessageRequestClientCapabilities(message_action_item=ShowMessageRequestClientCapabilitiesMessageActionItemType(additional_properties_support=False)), show_document=ShowDocumentClientCapabilities(support=True)), general=GeneralClientCapabilities(stale_request_support=None, regular_expressions=None, markdown=None, position_encodings=['utf-16']), experimental=None), process_id=986970, client_info=InitializeParamsClientInfoType(name='Neovim', version='0.10.4+v0.10.4'), locale=None, root_path='/home/elmortti/.config/nvim', root_uri='file:///home/elmortti/.config/nvim', initialization_options={'stream_suggestion': False, 'ollama_model_opts': {'num_predict': 40, 'temperature': 0.1}, 'model_name': 'deepseek-coder:base'}, trace=<TraceValues.Off: 'off'>, work_done_token='1', workspace_folders=[WorkspaceFolder(uri='file:///home/elmortti/.config/nvim', name='/home/elmortti/.config/nvim')])
2025-02-02 06:22:45,693 - pygls.protocol.language_server - DEBUG - Server capabilities: {"positionEncoding": "utf-16", "textDocumentSync": {"openClose": true, "change": 2, "willSave": false, "willSaveWaitUntil": false, "save": false}, "completionProvider": {}, "executeCommandProvider": {"commands": []}, "workspace": {"workspaceFolders": {"supported": true, "changeNotifications": true}, "fileOperations": {}}}
2025-02-02 06:22:45,693 - ollama_copilot - INFO - Initializing Ollama LSP server
2025-02-02 06:22:45,697 - ollama_copilot - INFO - Initialized with model: deepseek-coder:base
2025-02-02 06:22:45,699 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 1, "jsonrpc": "2.0", "result": {"capabilities": {"positionEncoding": "utf-16", "textDocumentSync": {"openClose": true, "change": 2, "willSave": false, "willSaveWaitUntil": false, "save": false}, "completionProvider": {}, "executeCommandProvider": {"commands": []}, "workspace": {"workspaceFolders": {"supported": true, "changeNotifications": true}, "fileOperations": {}}}, "serverInfo": {"name": "ollama-server", "version": "v0.3"}}}
2025-02-02 06:22:45,699 - pygls.server - DEBUG - Content length: 52
2025-02-02 06:22:45,699 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 52\r\n\r\n{"method":"initialized","jsonrpc":"2.0","params":{}}'
2025-02-02 06:22:45,700 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:22:45,700 - pygls.server - DEBUG - Content length: 676
2025-02-02 06:22:45,700 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 676\r\n\r\n{"method":"textDocument\\/didOpen","jsonrpc":"2.0","params":{"textDocument":{"languageId":"lua","uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua","text":"-- Custom configuration (defaults shown)\\nreturn {\\n\\t\'jasper-clarke\\/collama.nvim\',\\n\\topts = {\\n\\t\\tmodel_name = \\"deepseek-coder:1.3b\\",\\n\\t\\tstream_suggestion = true,\\n\\t\\tpython_command = \\"python3\\",\\n\\t\\tfiletypes = { \'python\', \'lua\', \'vim\', \\"markdown\\" },\\n\\t\\tollama_model_opts = {\\n\\t\\t\\tnum_predict = 40,\\n\\t\\t\\ttemperature = 0.1,\\n\\t\\t},\\n\\t\\tkeymaps = {\\n\\t\\t\\tsuggestion = \'<leader>os\',\\n\\t\\t\\treject = \'<leader>or\',\\n\\t\\t\\tinsert_accept = \'<Tab>\',\\n\\t\\t},\\n\\t}\\n}\\n","version":0}}}'
2025-02-02 06:22:45,701 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:22:48,147 - pygls.server - DEBUG - Content length: 302
2025-02-02 06:22:48,147 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 302\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":14,"character":25},"end":{"line":15,"character":0}},"text":"\\n\\t\\t\\t\\n","rangeLength":1}],"textDocument":{"version":3,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:22:48,150 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:22:48,150 - ollama_copilot - DEBUG - Document change detected: 
			

2025-02-02 06:22:48,477 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:22:48,477 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":15,"character":3},"end":{"line":15,"character":3}},"text":"a","rangeLength":0}],"textDocument":{"version":4,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:22:48,477 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:22:48,477 - ollama_copilot - DEBUG - Document change detected: a
2025-02-02 06:22:48,477 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:22:48,477 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:22:48,479 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:22:48,480 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"id":2,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":15,"character":4},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:22:48,481 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:22:48,481 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 2, "jsonrpc": "2.0", "result": []}
2025-02-02 06:22:48,698 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:22:48,698 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":15,"character":4},"end":{"line":15,"character":4}},"text":"c","rangeLength":0}],"textDocument":{"version":5,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:22:48,698 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:22:48,698 - ollama_copilot - DEBUG - Document change detected: c
2025-02-02 06:22:48,698 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:22:48,699 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:22:48,903 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:22:48,903 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":15,"character":5},"end":{"line":15,"character":5}},"text":"c","rangeLength":0}],"textDocument":{"version":6,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:22:48,904 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:22:48,904 - ollama_copilot - DEBUG - Document change detected: c
2025-02-02 06:22:48,904 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:22:48,904 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:22:49,405 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:22:49,405 - ollama_copilot - DEBUG - Position - Line: 15, Character: 6
2025-02-02 06:22:49,407 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
2025-02-02 06:22:49,411 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x744acbb7d2b0>
2025-02-02 06:22:49,411 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:22:49,411 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:22:49,411 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:22:49,411 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:22:49,411 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:22:49,451 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:22:49 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:22:49,451 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:22:49,451 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:22:49,451 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:22:49,451 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:22:49,451 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:22:49,451 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 16, 'character': 6, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:22:49,451 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 16, "character": 6, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:22:49,451 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:22:49,515 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:22:49,516 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":15,"character":6},"end":{"line":15,"character":6}},"text":"e","rangeLength":0}],"textDocument":{"version":7,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:22:49,516 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:22:49,517 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:22:49,517 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:22:49,517 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:22:50,018 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:22:50,018 - ollama_copilot - DEBUG - Position - Line: 15, Character: 7
2025-02-02 06:22:50,020 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:22:50,020 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:22:50,021 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:22:50,021 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:22:50,021 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:22:50,060 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:22:50 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:22:50,060 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:22:50,060 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:22:50,061 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:22:50,061 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:22:50,061 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:22:50,061 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 16, 'character': 7, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:22:50,061 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 16, "character": 7, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:22:50,061 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:22:51,935 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:22:51,936 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":15,"character":7},"end":{"line":15,"character":7}},"text":"p","rangeLength":0}],"textDocument":{"version":8,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:22:51,936 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:22:51,937 - ollama_copilot - DEBUG - Document change detected: p
2025-02-02 06:22:51,937 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:22:51,937 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:22:52,083 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:22:52,084 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":15,"character":8},"end":{"line":15,"character":8}},"text":"t","rangeLength":0}],"textDocument":{"version":9,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:22:52,084 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:22:52,084 - ollama_copilot - DEBUG - Document change detected: t
2025-02-02 06:22:52,084 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:22:52,084 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:22:52,424 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:22:52,426 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":15,"character":9},"end":{"line":15,"character":9}},"text":" ","rangeLength":0}],"textDocument":{"version":10,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:22:52,426 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:22:52,426 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:22:52,427 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:22:52,427 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:22:52,928 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:22:52,928 - ollama_copilot - DEBUG - Position - Line: 15, Character: 10
2025-02-02 06:22:52,929 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:22:52,930 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:22:52,930 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:22:52,930 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:22:52,930 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:22:52,969 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:22:52 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:22:52,970 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:22:52,970 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:22:52,970 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:22:52,970 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:22:52,970 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:22:52,970 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 16, 'character': 10, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:22:52,970 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 16, "character": 10, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:22:52,970 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:22:55,365 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:22:55,365 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":15,"character":0},"end":{"line":16,"character":0}},"text":"","rangeLength":11}],"textDocument":{"version":11,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:22:55,366 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:22:55,366 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:23:01,450 - pygls.server - DEBUG - Content length: 310
2025-02-02 06:23:01,450 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 310\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":14,"character":25},"end":{"line":15,"character":0}},"text":"\\n\\t\\t\\taccept \\n","rangeLength":1}],"textDocument":{"version":12,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:23:01,450 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:23:01,450 - ollama_copilot - DEBUG - Document change detected: 
			accept 

2025-02-02 06:23:01,458 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:23:01,458 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":15,"character":0},"end":{"line":16,"character":0}},"text":"","rangeLength":11}],"textDocument":{"version":14,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:23:01,459 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:23:01,459 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:23:03,417 - pygls.server - DEBUG - Content length: 310
2025-02-02 06:23:03,418 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 310\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":14,"character":25},"end":{"line":15,"character":0}},"text":"\\n\\t\\t\\taccept \\n","rangeLength":1}],"textDocument":{"version":16,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:23:03,418 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:23:03,418 - ollama_copilot - DEBUG - Document change detected: 
			accept 

2025-02-02 06:23:03,567 - pygls.server - DEBUG - Content length: 1925
2025-02-02 06:23:03,567 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 1925\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":15,"character":3},"end":{"line":15,"character":10}},"text":"","rangeLength":7},{"range":{"start":{"line":15,"character":0},"end":{"line":16,"character":0}},"text":"","rangeLength":4},{"range":{"start":{"line":14,"character":25},"end":{"line":15,"character":0}},"text":"\\n\\t\\t\\t\\n","rangeLength":1},{"range":{"start":{"line":15,"character":3},"end":{"line":15,"character":3}},"text":"accept","rangeLength":0},{"range":{"start":{"line":15,"character":9},"end":{"line":15,"character":9}},"text":" = ","rangeLength":0},{"range":{"start":{"line":2,"character":30},"end":{"line":3,"character":0}},"text":"\\n","rangeLength":1},{"range":{"start":{"line":2,"character":2},"end":{"line":2,"character":15}},"text":"Jacob411","rangeLength":13},{"range":{"start":{"line":15,"character":0},"end":{"line":16,"character":0}},"text":"","rangeLength":13},{"range":{"start":{"line":2,"character":11},"end":{"line":2,"character":18}},"text":"Ollama-Copilot","rangeLength":7},{"range":{"start":{"line":2,"character":26},"end":{"line":2,"character":30}},"text":"","rangeLength":4},{"range":{"start":{"line":2,"character":25},"end":{"line":2,"character":26}},"text":"","rangeLength":1},{"range":{"start":{"line":14,"character":25},"end":{"line":15,"character":0}},"text":"\\n\\t\\t\\t\\n","rangeLength":1},{"range":{"start":{"line":15,"character":3},"end":{"line":15,"character":3}},"text":"acc","rangeLength":0},{"range":{"start":{"line":15,"character":0},"end":{"line":16,"character":0}},"text":"","rangeLength":7},{"range":{"start":{"line":14,"character":25},"end":{"line":15,"character":0}},"text":"\\n\\t\\t\\tacc\\n","rangeLength":1},{"range":{"start":{"line":15,"character":0},"end":{"line":16,"character":0}},"text":"","rangeLength":7}],"textDocument":{"version":44,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:23:03,567 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:23:03,567 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:24:09,361 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:24:09,361 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":0,"character":0},"end":{"line":19,"character":0}},"text":"","rangeLength":418}],"textDocument":{"version":47,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:09,361 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:09,361 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:24:10,653 - pygls.server - DEBUG - Content length: 770
2025-02-02 06:24:10,653 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 770\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":0,"character":0},"end":{"line":1,"character":0}},"text":"\\n-- Custom configuration (defaults shown)\\nreturn {\\n\\t\'Jacob411\\/Ollama-Copilot\',\\n\\topts = {\\n\\t\\tmodel_name = \\"deepseek-coder:1.3b\\",\\n\\t\\tstream_suggestion = true,\\n\\t\\tpython_command = \\"python3\\",\\n\\t\\tfiletypes = { \'python\', \'lua\', \'vim\', \\"markdown\\" },\\n\\t\\tollama_model_opts = {\\n\\t\\t\\tnum_predict = 40,\\n\\t\\t\\ttemperature = 0.1,\\n\\t\\t},\\n\\t\\tkeymaps = {\\n\\t\\t\\tsuggestion = \'<leader>os\',\\n\\t\\t\\treject = \'<leader>or\',\\n\\t\\t\\tinsert_accept = \'<Tab>\',\\n\\t\\t},\\n\\t}\\n}\\n","rangeLength":1}],"textDocument":{"version":48,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:10,654 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:10,654 - ollama_copilot - DEBUG - Document change detected: 
-- Custom configuration (defaults shown)
return {
	'Jacob411/Ollama-Copilot',
	opts = {
		model_name = "deepseek-coder:1.3b",
		stream_suggestion = true,
		python_command = "python3",
		filetypes = { 'python', 'lua', 'vim', "markdown" },
		ollama_model_opts = {
			num_predict = 40,
			temperature = 0.1,
		},
		keymaps = {
			suggestion = '<leader>os',
			reject = '<leader>or',
			insert_accept = '<Tab>',
		},
	}
}

2025-02-02 06:24:11,958 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:24:11,958 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":0,"character":0},"end":{"line":19,"character":1}},"text":"","rangeLength":418}],"textDocument":{"version":49,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:11,958 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:11,959 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:24:11,960 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:24:11,960 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":0,"character":0},"end":{"line":1,"character":0}},"text":"\\n","rangeLength":1}],"textDocument":{"version":50,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:11,960 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:11,961 - ollama_copilot - DEBUG - Document change detected: 

2025-02-02 06:24:13,141 - pygls.server - DEBUG - Content length: 1119
2025-02-02 06:24:13,141 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 1119\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":0,"character":0},"end":{"line":0,"character":0}},"text":"return {\\n  \\"nomnivore\\/ollama.nvim\\",\\n  dependencies = {\\n    \\"nvim-lua\\/plenary.nvim\\",\\n  },\\n\\n  -- All the user commands added by the plugin\\n  cmd = { \\"Ollama\\", \\"OllamaModel\\", \\"OllamaServe\\", \\"OllamaServeStop\\" },\\n\\n  keys = {\\n    -- Sample keybind for prompt menu. Note that the <c-u> is important for selections to work properly.\\n    {\\n      \\"<leader>oo\\",\\n      \\":<c-u>lua require(\'ollama\').prompt()<cr>\\",\\n      desc = \\"ollama prompt\\",\\n      mode = { \\"n\\", \\"v\\" },\\n    },\\n\\n    -- Sample keybind for direct prompting. Note that the <c-u> is important for selections to work properly.\\n    {\\n      \\"<leader>oG\\",\\n      \\":<c-u>lua require(\'ollama\').prompt(\'Generate_Code\')<cr>\\",\\n      desc = \\"ollama Generate Code\\",\\n      mode = { \\"n\\", \\"v\\" },\\n    },\\n  },\\n\\n  ---@type Ollama.Config","rangeLength":0}],"textDocument":{"version":52,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:13,142 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:13,142 - ollama_copilot - DEBUG - Document change detected: return {
  "nomnivore/ollama.nvim",
  dependencies = {
    "nvim-lua/plenary.nvim",
  },

  -- All the user commands added by the plugin
  cmd = { "Ollama", "OllamaModel", "OllamaServe", "OllamaServeStop" },

  keys = {
    -- Sample keybind for prompt menu. Note that the <c-u> is important for selections to work properly.
    {
      "<leader>oo",
      ":<c-u>lua require('ollama').prompt()<cr>",
      desc = "ollama prompt",
      mode = { "n", "v" },
    },

    -- Sample keybind for direct prompting. Note that the <c-u> is important for selections to work properly.
    {
      "<leader>oG",
      ":<c-u>lua require('ollama').prompt('Generate_Code')<cr>",
      desc = "ollama Generate Code",
      mode = { "n", "v" },
    },
  },

  ---@type Ollama.Config
2025-02-02 06:24:13,162 - pygls.server - DEBUG - Content length: 351
2025-02-02 06:24:13,163 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 351\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":27,"character":24},"end":{"line":27,"character":24}},"text":"\\n  opts = {\\n    -- your configuration overrides\\n  }\\n}","rangeLength":0}],"textDocument":{"version":53,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:13,163 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:13,163 - ollama_copilot - DEBUG - Document change detected: 
  opts = {
    -- your configuration overrides
  }
}
2025-02-02 06:24:22,019 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:24:22,019 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":30,"character":2},"end":{"line":30,"character":3}},"text":"","rangeLength":1}],"textDocument":{"version":54,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:22,019 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:22,019 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:24:22,536 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:24:22,537 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":30,"character":2},"end":{"line":30,"character":2}},"text":"k","rangeLength":0}],"textDocument":{"version":55,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:22,537 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:22,537 - ollama_copilot - DEBUG - Document change detected: k
2025-02-02 06:24:22,538 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:24:22,538 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:24:22,548 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:24:22,548 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"id":3,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":30,"character":3},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:24:22,549 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:24:22,549 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 3, "jsonrpc": "2.0", "result": []}
2025-02-02 06:24:22,766 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:24:22,766 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":30,"character":3},"end":{"line":30,"character":3}},"text":"k","rangeLength":0}],"textDocument":{"version":56,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:22,766 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:22,766 - ollama_copilot - DEBUG - Document change detected: k
2025-02-02 06:24:22,766 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:24:22,766 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:24:23,001 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:24:23,002 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":30,"character":4},"end":{"line":30,"character":4}},"text":"k","rangeLength":0}],"textDocument":{"version":57,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:23,002 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:23,002 - ollama_copilot - DEBUG - Document change detected: k
2025-02-02 06:24:23,002 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:24:23,002 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:24:23,503 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:24:23,503 - ollama_copilot - DEBUG - Position - Line: 30, Character: 5
2025-02-02 06:24:23,504 - httpcore.connection - DEBUG - close.started
2025-02-02 06:24:23,505 - httpcore.connection - DEBUG - close.complete
2025-02-02 06:24:23,505 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
2025-02-02 06:24:23,506 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x744acbb86e90>
2025-02-02 06:24:23,506 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:24:23,506 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:24:23,506 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:24:23,506 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:24:23,507 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:24:23,638 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:24:23 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:24:23,638 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:24:23,638 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:24:23,638 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:24:23,638 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:24:23,638 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:24:23,638 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 31, 'character': 5, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:24:23,638 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 31, "character": 5, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:24:23,638 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:24:26,643 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:24:26,643 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":27,"character":0},"end":{"line":31,"character":0}},"text":"","rangeLength":78}],"textDocument":{"version":58,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:26,644 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:26,644 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:24:26,644 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:24:26,644 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:24:26,653 - pygls.server - DEBUG - Content length: 843
2025-02-02 06:24:26,653 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 843\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":27,"character":1},"end":{"line":27,"character":1}},"text":"opts = {\\n  model = \\"mistral\\",\\n  url = \\"http:\\/\\/127.0.0.1:11434\\",\\n  serve = {\\n    on_start = false,\\n    command = \\"ollama\\",\\n    args = { \\"serve\\" },\\n    stop_command = \\"pkill\\",\\n    stop_args = { \\"-SIGTERM\\", \\"ollama\\" },\\n  },\\n  -- View the actual default prompts in .\\/lua\\/ollama\\/prompts.lua\\n  prompts = {\\n    Sample_Prompt = {\\n      prompt = \\"This is a sample prompt that receives $input and $sel(ection), among others.\\",\\n      input_label = \\"> \\",\\n      model = \\"mistral\\",\\n      action = \\"display\\",\\n    }\\n  }\\n}","rangeLength":0}],"textDocument":{"version":59,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:26,653 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:26,653 - ollama_copilot - DEBUG - Document change detected: opts = {
  model = "mistral",
  url = "http://127.0.0.1:11434",
  serve = {
    on_start = false,
    command = "ollama",
    args = { "serve" },
    stop_command = "pkill",
    stop_args = { "-SIGTERM", "ollama" },
  },
  -- View the actual default prompts in ./lua/ollama/prompts.lua
  prompts = {
    Sample_Prompt = {
      prompt = "This is a sample prompt that receives $input and $sel(ection), among others.",
      input_label = "> ",
      model = "mistral",
      action = "display",
    }
  }
}
2025-02-02 06:24:33,629 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:24:33,631 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":27,"character":1},"end":{"line":46,"character":1}},"text":"","rangeLength":505}],"textDocument":{"version":60,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:33,631 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:33,631 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:24:33,638 - pygls.server - DEBUG - Content length: 482
2025-02-02 06:24:33,639 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 482\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":27,"character":1},"end":{"line":28,"character":0}},"text":"\\n","rangeLength":1},{"range":{"start":{"line":26,"character":0},"end":{"line":27,"character":0}},"text":"\\n  ---@type Ollama.Config\\n  opts = {\\n    -- your configuration overrides\\n  kkk\\n","rangeLength":1}],"textDocument":{"version":62,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:33,639 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:33,639 - ollama_copilot - DEBUG - Document change detected: 

2025-02-02 06:24:40,411 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:24:40,411 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":26,"character":0},"end":{"line":31,"character":0}},"text":"","rangeLength":79}],"textDocument":{"version":64,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:40,411 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:40,412 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:24:40,412 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:24:40,412 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:24:40,431 - pygls.server - DEBUG - Content length: 843
2025-02-02 06:24:40,431 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 843\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":26,"character":1},"end":{"line":26,"character":1}},"text":"opts = {\\n  model = \\"mistral\\",\\n  url = \\"http:\\/\\/127.0.0.1:11434\\",\\n  serve = {\\n    on_start = false,\\n    command = \\"ollama\\",\\n    args = { \\"serve\\" },\\n    stop_command = \\"pkill\\",\\n    stop_args = { \\"-SIGTERM\\", \\"ollama\\" },\\n  },\\n  -- View the actual default prompts in .\\/lua\\/ollama\\/prompts.lua\\n  prompts = {\\n    Sample_Prompt = {\\n      prompt = \\"This is a sample prompt that receives $input and $sel(ection), among others.\\",\\n      input_label = \\"> \\",\\n      model = \\"mistral\\",\\n      action = \\"display\\",\\n    }\\n  }\\n}","rangeLength":0}],"textDocument":{"version":65,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:40,431 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:40,431 - ollama_copilot - DEBUG - Document change detected: opts = {
  model = "mistral",
  url = "http://127.0.0.1:11434",
  serve = {
    on_start = false,
    command = "ollama",
    args = { "serve" },
    stop_command = "pkill",
    stop_args = { "-SIGTERM", "ollama" },
  },
  -- View the actual default prompts in ./lua/ollama/prompts.lua
  prompts = {
    Sample_Prompt = {
      prompt = "This is a sample prompt that receives $input and $sel(ection), among others.",
      input_label = "> ",
      model = "mistral",
      action = "display",
    }
  }
}
2025-02-02 06:24:45,827 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:24:45,828 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":26,"character":1},"end":{"line":26,"character":1}},"text":",","rangeLength":0}],"textDocument":{"version":66,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:45,828 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:45,828 - ollama_copilot - DEBUG - Document change detected: ,
2025-02-02 06:24:45,943 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:24:45,943 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":26,"character":2},"end":{"line":26,"character":2}},"text":" ","rangeLength":0}],"textDocument":{"version":67,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:45,943 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:45,943 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:24:46,479 - pygls.server - DEBUG - Content length: 303
2025-02-02 06:24:46,479 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 303\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":26,"character":3},"end":{"line":26,"character":11}},"text":"\\nopts = {","rangeLength":8}],"textDocument":{"version":68,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:46,479 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:46,479 - ollama_copilot - DEBUG - Document change detected: 
opts = {
2025-02-02 06:24:52,027 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:24:52,028 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":26,"character":2},"end":{"line":26,"character":3}},"text":"","rangeLength":1}],"textDocument":{"version":69,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:52,028 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:52,029 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:24:56,689 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:24:56,689 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":26,"character":0},"end":{"line":27,"character":0}},"text":"","rangeLength":3}],"textDocument":{"version":70,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:56,689 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:56,689 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:24:58,861 - pygls.server - DEBUG - Content length: 298
2025-02-02 06:24:58,862 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 298\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":45,"character":1},"end":{"line":46,"character":0}},"text":"\\n},\\n","rangeLength":1}],"textDocument":{"version":71,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:24:58,862 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:24:58,862 - ollama_copilot - DEBUG - Document change detected: 
},

2025-02-02 06:25:02,873 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:25:02,873 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":46,"character":0},"end":{"line":47,"character":0}},"text":"","rangeLength":3}],"textDocument":{"version":72,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:02,874 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:02,874 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:25:02,875 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:25:02,875 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":45,"character":1},"end":{"line":46,"character":0}},"text":"\\n","rangeLength":1}],"textDocument":{"version":73,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:02,875 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:02,875 - ollama_copilot - DEBUG - Document change detected: 

2025-02-02 06:25:03,079 - pygls.server - DEBUG - Content length: 298
2025-02-02 06:25:03,079 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 298\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":25,"character":4},"end":{"line":26,"character":0}},"text":"\\n},\\n","rangeLength":1}],"textDocument":{"version":75,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:03,080 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:03,080 - ollama_copilot - DEBUG - Document change detected: 
},

2025-02-02 06:25:03,489 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:25:03,490 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":26,"character":2},"end":{"line":26,"character":2}},"text":" ","rangeLength":0}],"textDocument":{"version":77,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:03,490 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:03,490 - ollama_copilot - DEBUG - Document change detected:  
2025-02-02 06:25:03,491 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:25:03,491 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:25:03,917 - pygls.server - DEBUG - Content length: 301
2025-02-02 06:25:03,918 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 301\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":26,"character":1},"end":{"line":27,"character":8}},"text":"opts = {","rangeLength":11}],"textDocument":{"version":79,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:03,919 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:03,919 - ollama_copilot - DEBUG - Document change detected: opts = {
2025-02-02 06:25:03,993 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:25:03,993 - ollama_copilot - DEBUG - Position - Line: 26, Character: 3
2025-02-02 06:25:03,994 - httpcore.connection - DEBUG - close.started
2025-02-02 06:25:03,994 - httpcore.connection - DEBUG - close.complete
2025-02-02 06:25:03,994 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
2025-02-02 06:25:03,994 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x744acbb87610>
2025-02-02 06:25:03,994 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:25:03,995 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:25:03,995 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:25:03,995 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:25:03,995 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:25:04,097 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:25:04 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:25:04,097 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:25:04,097 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:25:04,097 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:25:04,097 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:25:04,097 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:25:04,097 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 27, 'character': 3, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:25:04,097 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 27, "character": 3, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:25:04,097 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:25:04,423 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:25:04,424 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":26,"character":1},"end":{"line":45,"character":1}},"text":"","rangeLength":505}],"textDocument":{"version":81,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:04,424 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:04,424 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:25:04,426 - pygls.server - DEBUG - Content length: 484
2025-02-02 06:25:04,426 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 484\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":26,"character":1},"end":{"line":27,"character":0}},"text":"\\n","rangeLength":1},{"range":{"start":{"line":25,"character":4},"end":{"line":26,"character":0}},"text":"\\n\\n  ---@type Ollama.Config\\n  opts = {\\n    -- your configuration overrides\\n  kkk\\n","rangeLength":1}],"textDocument":{"version":83,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:04,426 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:04,426 - ollama_copilot - DEBUG - Document change detected: 

2025-02-02 06:25:05,697 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:25:05,697 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":30,"character":2},"end":{"line":30,"character":5}},"text":"}","rangeLength":3}],"textDocument":{"version":85,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:05,698 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:05,698 - ollama_copilot - DEBUG - Document change detected: }
2025-02-02 06:25:05,698 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:25:05,698 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:25:06,199 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:25:06,199 - ollama_copilot - DEBUG - Position - Line: 30, Character: 6
2025-02-02 06:25:06,200 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:25:06,200 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:25:06,200 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:25:06,200 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:25:06,201 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:25:06,235 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:25:06 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:25:06,235 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:25:06,235 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:25:06,235 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:25:06,235 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:25:06,235 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:25:06,235 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 31, 'character': 6, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:25:06,235 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 31, "character": 6, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:25:06,235 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:25:25,767 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:25:25,767 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":29,"character":0},"end":{"line":30,"character":0}},"text":"","rangeLength":36}],"textDocument":{"version":87,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:25,768 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:25,768 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:25:27,851 - pygls.server - DEBUG - Content length: 299
2025-02-02 06:25:27,852 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 299\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":10},"end":{"line":29,"character":0}},"text":"\\n  \\n","rangeLength":1}],"textDocument":{"version":88,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:27,852 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:27,852 - ollama_copilot - DEBUG - Document change detected: 
  

2025-02-02 06:25:27,995 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:25:27,996 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":29,"character":0},"end":{"line":29,"character":0}},"text":"\\t","rangeLength":0}],"textDocument":{"version":89,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:27,997 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:27,997 - ollama_copilot - DEBUG - Document change detected: 	
2025-02-02 06:25:27,997 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/clearSuggestion' {'message': 'clear current'}
2025-02-02 06:25:27,997 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/clearSuggestion", "jsonrpc": "2.0", "params": {"message": "clear current"}}
2025-02-02 06:25:28,498 - ollama_copilot - INFO - Starting completion for file: /home/elmortti/.config/nvim/lua/plugins/collama.lua
2025-02-02 06:25:28,498 - ollama_copilot - DEBUG - Position - Line: 29, Character: 1
2025-02-02 06:25:28,499 - httpcore.connection - DEBUG - close.started
2025-02-02 06:25:28,499 - httpcore.connection - DEBUG - close.complete
2025-02-02 06:25:28,499 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None
2025-02-02 06:25:28,499 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x744acbba4b00>
2025-02-02 06:25:28,499 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-02-02 06:25:28,499 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-02-02 06:25:28,499 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-02-02 06:25:28,499 - httpcore.http11 - DEBUG - send_request_body.complete
2025-02-02 06:25:28,499 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-02-02 06:25:28,529 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Sun, 02 Feb 2025 04:25:28 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-02-02 06:25:28,529 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/chat "HTTP/1.1 200 OK"
2025-02-02 06:25:28,529 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-02-02 06:25:28,529 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-02-02 06:25:28,529 - httpcore.http11 - DEBUG - response_closed.started
2025-02-02 06:25:28,529 - httpcore.http11 - DEBUG - response_closed.complete
2025-02-02 06:25:28,529 - pygls.protocol.json_rpc - DEBUG - Sending notification: '$/tokenStream' {'line': 30, 'character': 1, 'completion': {'total': '', 'type': 'completion'}}
2025-02-02 06:25:28,529 - pygls.protocol.json_rpc - INFO - Sending data: {"method": "$/tokenStream", "jsonrpc": "2.0", "params": {"line": 30, "character": 1, "completion": {"total": "", "type": "completion"}}}
2025-02-02 06:25:28,529 - ollama_copilot - INFO - Completion finished successfully. Length: 0
2025-02-02 06:25:28,907 - pygls.server - DEBUG - Content length: 828
2025-02-02 06:25:28,908 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 828\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":29,"character":3},"end":{"line":29,"character":3}},"text":"model = \\"mistral\\",\\n  url = \\"http:\\/\\/127.0.0.1:11434\\",\\n  serve = {\\n    on_start = false,\\n    command = \\"ollama\\",\\n    args = { \\"serve\\" },\\n    stop_command = \\"pkill\\",\\n    stop_args = { \\"-SIGTERM\\", \\"ollama\\" },\\n  },\\n  -- View the actual default prompts in .\\/lua\\/ollama\\/prompts.lua\\n  prompts = {\\n    Sample_Prompt = {\\n      prompt = \\"This is a sample prompt that receives $input and $sel(ection), among others.\\",\\n      input_label = \\"> \\",\\n      model = \\"mistral\\",\\n      action = \\"display\\",\\n    }\\n  }","rangeLength":0}],"textDocument":{"version":90,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:28,909 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:28,909 - ollama_copilot - DEBUG - Document change detected: model = "mistral",
  url = "http://127.0.0.1:11434",
  serve = {
    on_start = false,
    command = "ollama",
    args = { "serve" },
    stop_command = "pkill",
    stop_args = { "-SIGTERM", "ollama" },
  },
  -- View the actual default prompts in ./lua/ollama/prompts.lua
  prompts = {
    Sample_Prompt = {
      prompt = "This is a sample prompt that receives $input and $sel(ection), among others.",
      input_label = "> ",
      model = "mistral",
      action = "display",
    }
  }
2025-02-02 06:25:32,130 - pygls.server - DEBUG - Content length: 1713
2025-02-02 06:25:32,130 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 1713\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":0,"character":8},"end":{"line":48,"character":1}},"text":"\\n\\t\\"nomnivore\\/ollama.nvim\\",\\n\\tdependencies = {\\n\\t\\t\\"nvim-lua\\/plenary.nvim\\",\\n\\t},\\n\\n\\t-- All the user commands added by the plugin\\n\\tcmd = { \\"Ollama\\", \\"OllamaModel\\", \\"OllamaServe\\", \\"OllamaServeStop\\" },\\n\\n\\tkeys = {\\n\\t\\t-- Sample keybind for prompt menu. Note that the <c-u> is important for selections to work properly.\\n\\t\\t{\\n\\t\\t\\t\\"<leader>oo\\",\\n\\t\\t\\t\\":<c-u>lua require(\'ollama\').prompt()<cr>\\",\\n\\t\\t\\tdesc = \\"ollama prompt\\",\\n\\t\\t\\tmode = { \\"n\\", \\"v\\" },\\n\\t\\t},\\n\\n\\t\\t-- Sample keybind for direct prompting. Note that the <c-u> is important for selections to work properly.\\n\\t\\t{\\n\\t\\t\\t\\"<leader>oG\\",\\n\\t\\t\\t\\":<c-u>lua require(\'ollama\').prompt(\'Generate_Code\')<cr>\\",\\n\\t\\t\\tdesc = \\"ollama Generate Code\\",\\n\\t\\t\\tmode = { \\"n\\", \\"v\\" },\\n\\t\\t},\\n\\t},\\n\\n\\t---@type Ollama.Config\\n\\topts = {\\n\\t\\tmodel = \\"mistral\\",\\n\\t\\turl = \\"http:\\/\\/127.0.0.1:11434\\",\\n\\t\\tserve = {\\n\\t\\t\\ton_start = false,\\n\\t\\t\\tcommand = \\"ollama\\",\\n\\t\\t\\targs = { \\"serve\\" },\\n\\t\\t\\tstop_command = \\"pkill\\",\\n\\t\\t\\tstop_args = { \\"-SIGTERM\\", \\"ollama\\" },\\n\\t\\t},\\n\\t\\t-- View the actual default prompts in .\\/lua\\/ollama\\/prompts.lua\\n\\t\\tprompts = {\\n\\t\\t\\tSample_Prompt = {\\n\\t\\t\\t\\tprompt = \\"This is a sample prompt that receives $input and $sel(ection), among others.\\",\\n\\t\\t\\t\\tinput_label = \\"> \\",\\n\\t\\t\\t\\tmodel = \\"mistral\\",\\n\\t\\t\\t\\taction = \\"display\\",\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n","rangeLength":1273}],"textDocument":{"version":91,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:32,130 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:32,130 - ollama_copilot - DEBUG - Document change detected: 
	"nomnivore/ollama.nvim",
	dependencies = {
		"nvim-lua/plenary.nvim",
	},

	-- All the user commands added by the plugin
	cmd = { "Ollama", "OllamaModel", "OllamaServe", "OllamaServeStop" },

	keys = {
		-- Sample keybind for prompt menu. Note that the <c-u> is important for selections to work properly.
		{
			"<leader>oo",
			":<c-u>lua require('ollama').prompt()<cr>",
			desc = "ollama prompt",
			mode = { "n", "v" },
		},

		-- Sample keybind for direct prompting. Note that the <c-u> is important for selections to work properly.
		{
			"<leader>oG",
			":<c-u>lua require('ollama').prompt('Generate_Code')<cr>",
			desc = "ollama Generate Code",
			mode = { "n", "v" },
		},
	},

	---@type Ollama.Config
	opts = {
		model = "mistral",
		url = "http://127.0.0.1:11434",
		serve = {
			on_start = false,
			command = "ollama",
			args = { "serve" },
			stop_command = "pkill",
			stop_args = { "-SIGTERM", "ollama" },
		},
		-- View the actual default prompts in ./lua/ollama/prompts.lua
		prompts = {
			Sample_Prompt = {
				prompt = "This is a sample prompt that receives $input and $sel(ection), among others.",
				input_label = "> ",
				model = "mistral",
				action = "display",
			}
		}
	}
}

2025-02-02 06:25:32,134 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:25:32,134 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":49,"character":0},"end":{"line":50,"character":0}},"text":"","rangeLength":1}],"textDocument":{"version":92,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:32,134 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:32,134 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:25:35,723 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:25:35,724 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":27,"character":0},"end":{"line":27,"character":1}},"text":"","rangeLength":1}],"textDocument":{"version":93,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:35,724 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:35,724 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:25:35,910 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:25:35,910 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":27,"character":0},"end":{"line":27,"character":0}},"text":"s","rangeLength":0}],"textDocument":{"version":94,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:35,910 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:35,910 - ollama_copilot - DEBUG - Document change detected: s
2025-02-02 06:25:35,915 - pygls.server - DEBUG - Content length: 229
2025-02-02 06:25:35,915 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 229\r\n\r\n{"id":4,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":27,"character":1},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:25:35,915 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:25:35,915 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 4, "jsonrpc": "2.0", "result": []}
2025-02-02 06:25:37,388 - pygls.server - DEBUG - Content length: 293
2025-02-02 06:25:37,389 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 293\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":27,"character":0},"end":{"line":28,"character":0}},"text":"","rangeLength":24}],"textDocument":{"version":95,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:37,389 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:37,389 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:25:39,125 - pygls.server - DEBUG - Content length: 1687
2025-02-02 06:25:39,125 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 1687\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":0,"character":8},"end":{"line":47,"character":1}},"text":"\\n\\t\\"nomnivore\\/ollama.nvim\\",\\n\\tdependencies = {\\n\\t\\t\\"nvim-lua\\/plenary.nvim\\",\\n\\t},\\n\\n\\t-- All the user commands added by the plugin\\n\\tcmd = { \\"Ollama\\", \\"OllamaModel\\", \\"OllamaServe\\", \\"OllamaServeStop\\" },\\n\\n\\tkeys = {\\n\\t\\t-- Sample keybind for prompt menu. Note that the <c-u> is important for selections to work properly.\\n\\t\\t{\\n\\t\\t\\t\\"<leader>oo\\",\\n\\t\\t\\t\\":<c-u>lua require(\'ollama\').prompt()<cr>\\",\\n\\t\\t\\tdesc = \\"ollama prompt\\",\\n\\t\\t\\tmode = { \\"n\\", \\"v\\" },\\n\\t\\t},\\n\\n\\t\\t-- Sample keybind for direct prompting. Note that the <c-u> is important for selections to work properly.\\n\\t\\t{\\n\\t\\t\\t\\"<leader>oG\\",\\n\\t\\t\\t\\":<c-u>lua require(\'ollama\').prompt(\'Generate_Code\')<cr>\\",\\n\\t\\t\\tdesc = \\"ollama Generate Code\\",\\n\\t\\t\\tmode = { \\"n\\", \\"v\\" },\\n\\t\\t},\\n\\t},\\n\\n\\topts = {\\n\\t\\tmodel = \\"mistral\\",\\n\\t\\turl = \\"http:\\/\\/127.0.0.1:11434\\",\\n\\t\\tserve = {\\n\\t\\t\\ton_start = false,\\n\\t\\t\\tcommand = \\"ollama\\",\\n\\t\\t\\targs = { \\"serve\\" },\\n\\t\\t\\tstop_command = \\"pkill\\",\\n\\t\\t\\tstop_args = { \\"-SIGTERM\\", \\"ollama\\" },\\n\\t\\t},\\n\\t\\t-- View the actual default prompts in .\\/lua\\/ollama\\/prompts.lua\\n\\t\\tprompts = {\\n\\t\\t\\tSample_Prompt = {\\n\\t\\t\\t\\tprompt = \\"This is a sample prompt that receives $input and $sel(ection), among others.\\",\\n\\t\\t\\t\\tinput_label = \\"> \\",\\n\\t\\t\\t\\tmodel = \\"mistral\\",\\n\\t\\t\\t\\taction = \\"display\\",\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n","rangeLength":1185}],"textDocument":{"version":96,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:39,125 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:39,125 - ollama_copilot - DEBUG - Document change detected: 
	"nomnivore/ollama.nvim",
	dependencies = {
		"nvim-lua/plenary.nvim",
	},

	-- All the user commands added by the plugin
	cmd = { "Ollama", "OllamaModel", "OllamaServe", "OllamaServeStop" },

	keys = {
		-- Sample keybind for prompt menu. Note that the <c-u> is important for selections to work properly.
		{
			"<leader>oo",
			":<c-u>lua require('ollama').prompt()<cr>",
			desc = "ollama prompt",
			mode = { "n", "v" },
		},

		-- Sample keybind for direct prompting. Note that the <c-u> is important for selections to work properly.
		{
			"<leader>oG",
			":<c-u>lua require('ollama').prompt('Generate_Code')<cr>",
			desc = "ollama Generate Code",
			mode = { "n", "v" },
		},
	},

	opts = {
		model = "mistral",
		url = "http://127.0.0.1:11434",
		serve = {
			on_start = false,
			command = "ollama",
			args = { "serve" },
			stop_command = "pkill",
			stop_args = { "-SIGTERM", "ollama" },
		},
		-- View the actual default prompts in ./lua/ollama/prompts.lua
		prompts = {
			Sample_Prompt = {
				prompt = "This is a sample prompt that receives $input and $sel(ection), among others.",
				input_label = "> ",
				model = "mistral",
				action = "display",
			}
		}
	}
}

2025-02-02 06:25:39,129 - pygls.server - DEBUG - Content length: 292
2025-02-02 06:25:39,129 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 292\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":48,"character":0},"end":{"line":49,"character":0}},"text":"","rangeLength":1}],"textDocument":{"version":97,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:39,129 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:39,129 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:25:50,142 - pygls.server - DEBUG - Content length: 294
2025-02-02 06:25:50,142 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 294\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":11},"end":{"line":28,"character":18}},"text":"","rangeLength":7}],"textDocument":{"version":98,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:50,142 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:50,143 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:25:50,332 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:25:50,332 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":11},"end":{"line":28,"character":11}},"text":"d","rangeLength":0}],"textDocument":{"version":99,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:50,332 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:50,332 - ollama_copilot - DEBUG - Document change detected: d
2025-02-02 06:25:50,344 - pygls.server - DEBUG - Content length: 230
2025-02-02 06:25:50,344 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 230\r\n\r\n{"id":5,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":28,"character":12},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:25:50,344 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:25:50,344 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 5, "jsonrpc": "2.0", "result": []}
2025-02-02 06:25:50,557 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:50,557 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":12},"end":{"line":28,"character":12}},"text":"e","rangeLength":0}],"textDocument":{"version":100,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:50,557 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:50,557 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:25:50,711 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:50,711 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":13},"end":{"line":28,"character":13}},"text":"e","rangeLength":0}],"textDocument":{"version":101,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:50,711 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:50,711 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:25:50,860 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:50,860 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":14},"end":{"line":28,"character":14}},"text":"p","rangeLength":0}],"textDocument":{"version":102,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:50,860 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:50,860 - ollama_copilot - DEBUG - Document change detected: p
2025-02-02 06:25:51,009 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:51,009 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":15},"end":{"line":28,"character":15}},"text":"s","rangeLength":0}],"textDocument":{"version":103,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:51,010 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:51,010 - ollama_copilot - DEBUG - Document change detected: s
2025-02-02 06:25:51,161 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:51,161 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":16},"end":{"line":28,"character":16}},"text":"e","rangeLength":0}],"textDocument":{"version":104,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:51,162 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:51,162 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:25:51,309 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:51,310 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":17},"end":{"line":28,"character":17}},"text":"e","rangeLength":0}],"textDocument":{"version":105,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:51,310 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:51,310 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:25:51,459 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:51,460 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":18},"end":{"line":28,"character":18}},"text":"k","rangeLength":0}],"textDocument":{"version":106,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:51,461 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:51,461 - ollama_copilot - DEBUG - Document change detected: k
2025-02-02 06:25:51,697 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:51,698 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":19},"end":{"line":28,"character":19}},"text":"-","rangeLength":0}],"textDocument":{"version":107,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:51,699 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:51,699 - ollama_copilot - DEBUG - Document change detected: -
2025-02-02 06:25:51,941 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:51,942 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":20},"end":{"line":28,"character":20}},"text":"c","rangeLength":0}],"textDocument":{"version":108,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:51,942 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:51,942 - ollama_copilot - DEBUG - Document change detected: c
2025-02-02 06:25:52,090 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:52,090 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":21},"end":{"line":28,"character":21}},"text":"o","rangeLength":0}],"textDocument":{"version":109,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:52,091 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:52,091 - ollama_copilot - DEBUG - Document change detected: o
2025-02-02 06:25:52,241 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:52,242 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":22},"end":{"line":28,"character":22}},"text":"d","rangeLength":0}],"textDocument":{"version":110,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:52,243 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:52,243 - ollama_copilot - DEBUG - Document change detected: d
2025-02-02 06:25:52,389 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:52,390 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":23},"end":{"line":28,"character":23}},"text":"e","rangeLength":0}],"textDocument":{"version":111,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:52,390 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:52,390 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:25:52,587 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:52,588 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":24},"end":{"line":28,"character":24}},"text":":","rangeLength":0}],"textDocument":{"version":112,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:52,589 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:52,589 - ollama_copilot - DEBUG - Document change detected: :
2025-02-02 06:25:52,925 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:52,926 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":25},"end":{"line":28,"character":25}},"text":"1","rangeLength":0}],"textDocument":{"version":113,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:52,926 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:52,927 - ollama_copilot - DEBUG - Document change detected: 1
2025-02-02 06:25:52,942 - pygls.server - DEBUG - Content length: 230
2025-02-02 06:25:52,942 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 230\r\n\r\n{"id":6,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":28,"character":26},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:25:52,942 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:25:52,943 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 6, "jsonrpc": "2.0", "result": []}
2025-02-02 06:25:53,152 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:53,152 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":26},"end":{"line":28,"character":26}},"text":",","rangeLength":0}],"textDocument":{"version":114,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:53,152 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:53,153 - ollama_copilot - DEBUG - Document change detected: ,
2025-02-02 06:25:53,567 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:25:53,568 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":26},"end":{"line":28,"character":27}},"text":"","rangeLength":1}],"textDocument":{"version":115,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:53,569 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:53,569 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:25:53,581 - pygls.server - DEBUG - Content length: 230
2025-02-02 06:25:53,582 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 230\r\n\r\n{"id":7,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":28,"character":26},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:25:53,582 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:25:53,582 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 7, "jsonrpc": "2.0", "result": []}
2025-02-02 06:25:53,794 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:53,794 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":26},"end":{"line":28,"character":26}},"text":".","rangeLength":0}],"textDocument":{"version":116,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:53,794 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:53,794 - ollama_copilot - DEBUG - Document change detected: .
2025-02-02 06:25:54,103 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:54,103 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":27},"end":{"line":28,"character":27}},"text":"3","rangeLength":0}],"textDocument":{"version":117,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:54,103 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:54,103 - ollama_copilot - DEBUG - Document change detected: 3
2025-02-02 06:25:54,106 - pygls.server - DEBUG - Content length: 230
2025-02-02 06:25:54,106 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 230\r\n\r\n{"id":8,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":28,"character":28},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:25:54,106 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:25:54,106 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 8, "jsonrpc": "2.0", "result": []}
2025-02-02 06:25:54,407 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:54,407 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":28},"end":{"line":28,"character":28}},"text":"b","rangeLength":0}],"textDocument":{"version":118,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:54,408 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:54,408 - ollama_copilot - DEBUG - Document change detected: b
2025-02-02 06:25:54,413 - pygls.server - DEBUG - Content length: 230
2025-02-02 06:25:54,414 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 230\r\n\r\n{"id":9,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":28,"character":29},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:25:54,414 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:25:54,414 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 9, "jsonrpc": "2.0", "result": []}
2025-02-02 06:25:57,829 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:25:57,830 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":28,"character":24},"end":{"line":28,"character":24}},"text":"r","rangeLength":0}],"textDocument":{"version":119,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:25:57,830 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:25:57,831 - ollama_copilot - DEBUG - Document change detected: r
2025-02-02 06:25:57,844 - pygls.server - DEBUG - Content length: 231
2025-02-02 06:25:57,844 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 231\r\n\r\n{"id":10,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":28,"character":25},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:25:57,844 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:25:57,844 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 10, "jsonrpc": "2.0", "result": []}
2025-02-02 06:26:05,333 - pygls.server - DEBUG - Content length: 295
2025-02-02 06:26:05,333 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 295\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":13},"end":{"line":42,"character":20}},"text":"","rangeLength":7}],"textDocument":{"version":121,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:05,334 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:05,334 - ollama_copilot - DEBUG - Document change detected: 
2025-02-02 06:26:05,927 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:05,928 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":13},"end":{"line":42,"character":13}},"text":"d","rangeLength":0}],"textDocument":{"version":122,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:05,929 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:05,929 - ollama_copilot - DEBUG - Document change detected: d
2025-02-02 06:26:05,942 - pygls.server - DEBUG - Content length: 231
2025-02-02 06:26:05,942 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 231\r\n\r\n{"id":11,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":42,"character":14},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:26:05,942 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:26:05,942 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 11, "jsonrpc": "2.0", "result": []}
2025-02-02 06:26:06,076 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:06,076 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":14},"end":{"line":42,"character":14}},"text":"e","rangeLength":0}],"textDocument":{"version":123,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:06,077 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:06,077 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:26:06,240 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:06,241 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":15},"end":{"line":42,"character":15}},"text":"e","rangeLength":0}],"textDocument":{"version":124,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:06,241 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:06,241 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:26:06,425 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:06,426 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":16},"end":{"line":42,"character":16}},"text":"p","rangeLength":0}],"textDocument":{"version":125,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:06,427 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:06,427 - ollama_copilot - DEBUG - Document change detected: p
2025-02-02 06:26:06,575 - pygls.server - DEBUG - Content length: 403
2025-02-02 06:26:06,575 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 403\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":17},"end":{"line":42,"character":17}},"text":"s","rangeLength":0},{"range":{"start":{"line":42,"character":18},"end":{"line":42,"character":18}},"text":"e","rangeLength":0}],"textDocument":{"version":127,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:06,575 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:06,576 - ollama_copilot - DEBUG - Document change detected: s
2025-02-02 06:26:06,737 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:06,738 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":19},"end":{"line":42,"character":19}},"text":"e","rangeLength":0}],"textDocument":{"version":128,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:06,738 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:06,739 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:26:06,895 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:06,896 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":20},"end":{"line":42,"character":20}},"text":"k","rangeLength":0}],"textDocument":{"version":129,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:06,896 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:06,896 - ollama_copilot - DEBUG - Document change detected: k
2025-02-02 06:26:07,702 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:07,704 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":21},"end":{"line":42,"character":21}},"text":"-","rangeLength":0}],"textDocument":{"version":130,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:07,704 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:07,704 - ollama_copilot - DEBUG - Document change detected: -
2025-02-02 06:26:07,855 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:07,855 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":22},"end":{"line":42,"character":22}},"text":"c","rangeLength":0}],"textDocument":{"version":131,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:07,856 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:07,856 - ollama_copilot - DEBUG - Document change detected: c
2025-02-02 06:26:08,041 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:08,041 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":23},"end":{"line":42,"character":23}},"text":"o","rangeLength":0}],"textDocument":{"version":132,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:08,042 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:08,042 - ollama_copilot - DEBUG - Document change detected: o
2025-02-02 06:26:08,190 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:08,190 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":24},"end":{"line":42,"character":24}},"text":"d","rangeLength":0}],"textDocument":{"version":133,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:08,190 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:08,190 - ollama_copilot - DEBUG - Document change detected: d
2025-02-02 06:26:08,340 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:08,340 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":25},"end":{"line":42,"character":25}},"text":"e","rangeLength":0}],"textDocument":{"version":134,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:08,340 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:08,340 - ollama_copilot - DEBUG - Document change detected: e
2025-02-02 06:26:08,489 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:08,490 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":26},"end":{"line":42,"character":26}},"text":"r","rangeLength":0}],"textDocument":{"version":135,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:08,490 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:08,491 - ollama_copilot - DEBUG - Document change detected: r
2025-02-02 06:26:09,142 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:09,143 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":27},"end":{"line":42,"character":27}},"text":":","rangeLength":0}],"textDocument":{"version":136,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:09,144 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:09,144 - ollama_copilot - DEBUG - Document change detected: :
2025-02-02 06:26:09,402 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:09,403 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":28},"end":{"line":42,"character":28}},"text":"1","rangeLength":0}],"textDocument":{"version":137,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:09,403 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:09,404 - ollama_copilot - DEBUG - Document change detected: 1
2025-02-02 06:26:09,414 - pygls.server - DEBUG - Content length: 231
2025-02-02 06:26:09,415 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 231\r\n\r\n{"id":12,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":42,"character":29},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:26:09,415 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:26:09,415 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 12, "jsonrpc": "2.0", "result": []}
2025-02-02 06:26:09,645 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:09,645 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":29},"end":{"line":42,"character":29}},"text":".","rangeLength":0}],"textDocument":{"version":138,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:09,646 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:09,646 - ollama_copilot - DEBUG - Document change detected: .
2025-02-02 06:26:10,128 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:10,129 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":30},"end":{"line":42,"character":30}},"text":"3","rangeLength":0}],"textDocument":{"version":139,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:10,129 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:10,130 - ollama_copilot - DEBUG - Document change detected: 3
2025-02-02 06:26:10,137 - pygls.server - DEBUG - Content length: 231
2025-02-02 06:26:10,137 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 231\r\n\r\n{"id":13,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":42,"character":31},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:26:10,137 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:26:10,137 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 13, "jsonrpc": "2.0", "result": []}
2025-02-02 06:26:10,503 - pygls.server - DEBUG - Content length: 296
2025-02-02 06:26:10,504 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 296\r\n\r\n{"method":"textDocument\\/didChange","jsonrpc":"2.0","params":{"contentChanges":[{"range":{"start":{"line":42,"character":31},"end":{"line":42,"character":31}},"text":"b","rangeLength":0}],"textDocument":{"version":140,"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"}}}'
2025-02-02 06:26:10,504 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:10,504 - ollama_copilot - DEBUG - Document change detected: b
2025-02-02 06:26:10,516 - pygls.server - DEBUG - Content length: 231
2025-02-02 06:26:10,516 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 231\r\n\r\n{"id":14,"method":"textDocument\\/completion","jsonrpc":"2.0","params":{"position":{"line":42,"character":32},"textDocument":{"uri":"file:\\/\\/\\/home\\/elmortti\\/.config\\/nvim\\/lua\\/plugins\\/collama.lua"},"context":{"triggerKind":1}}}'
2025-02-02 06:26:10,517 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:26:10,517 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 14, "jsonrpc": "2.0", "result": []}
2025-02-02 06:26:27,054 - pygls.server - DEBUG - Content length: 45
2025-02-02 06:26:27,054 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 45\r\n\r\n{"id":15,"method":"shutdown","jsonrpc":"2.0"}'
2025-02-02 06:26:27,055 - pygls.protocol.json_rpc - DEBUG - Request message received.
2025-02-02 06:26:27,055 - pygls.protocol.json_rpc - INFO - Sending data: {"id": 15, "jsonrpc": "2.0", "result": null}
2025-02-02 06:26:27,059 - pygls.server - DEBUG - Content length: 33
2025-02-02 06:26:27,059 - pygls.protocol.json_rpc - DEBUG - Received b'Content-Length: 33\r\n\r\n{"method":"exit","jsonrpc":"2.0"}'
2025-02-02 06:26:27,060 - pygls.protocol.json_rpc - DEBUG - Notification message received.
2025-02-02 06:26:27,060 - pygls.server - INFO - Shutting down the server
2025-02-02 06:26:27,060 - pygls.server - INFO - Closing the event loop.
